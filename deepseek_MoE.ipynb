{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adf2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math \n",
    "from contextlib import nullcontext \n",
    "from typing import Optional \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0429ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Slightly faster GELU (approx)\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) *\n",
    "                                       (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class ExpertFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer MLP expert. Hidden dim is usually smaller than a dense FFN\n",
    "    (e.g., 0.25 × d_model in DeepSeek-V3).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, hidden: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.dropout(_gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSeek-V3 style Mixture-of-Experts (MoE) layer.\n",
    "\n",
    "    This MoE layer incorporates both routed experts (selected by a router)\n",
    "    and shared experts (applied to all inputs). It is designed based on\n",
    "    the architecture described in the DeepSeek-V3 paper.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output features.\n",
    "        n_routed_exp (int): The number of routed experts.\n",
    "        n_shared_exp (int, optional): The number of shared experts. Defaults to 1.\n",
    "        top_k (int, optional): The number of routed experts to select for each token.\n",
    "                               Defaults to 8.\n",
    "        routed_hidden (int, optional): The hidden dimension for routed experts.\n",
    "                                      Defaults to 2048.\n",
    "        shared_hidden (Optional[int], optional): The hidden dimension for shared experts.\n",
    "                                                If None, uses routed_hidden. Defaults to None.\n",
    "        bias_lr (float, optional): Learning rate for the router bias (updated online).\n",
    "                                   Defaults to 0.01.\n",
    "        fp16_router (bool, optional): Whether to use FP16 precision for router calculations.\n",
    "                                     Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_routed_exp: int,\n",
    "        n_shared_exp: int = 1,\n",
    "        top_k: int = 8,\n",
    "        routed_hidden: int = 2_048,\n",
    "        shared_hidden: Optional[int] = None,\n",
    "        bias_lr: float = 0.01,\n",
    "        fp16_router: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Assert that the number of selected experts (top_k) is less than or equal to the total number of routed experts.\n",
    "        assert top_k <= n_routed_exp, \"k must be ≤ number of routed experts\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_routed = n_routed_exp\n",
    "        self.n_shared = n_shared_exp\n",
    "        self.top_k = top_k\n",
    "        self.bias_lr = bias_lr\n",
    "        self.fp16_router = fp16_router\n",
    "\n",
    "        # Module list for the routed experts.\n",
    "        self.routed = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "        )\n",
    "        # Determine the hidden dimension for shared experts. Use routed_hidden if shared_hidden is not provided.\n",
    "        hidden_shared = shared_hidden or routed_hidden\n",
    "        # Module list for the shared experts.\n",
    "        self.shared = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, hidden_shared) for _ in range(n_shared_exp)]\n",
    "        )\n",
    "\n",
    "        # Register a parameter for the centroids used by the router.\n",
    "        # Centroids represent the \"preference\" of each expert for different input features.\n",
    "        self.register_parameter(\"centroids\", nn.Parameter(torch.empty(n_routed_exp, d_model)))\n",
    "        # Initialize centroids with a normal distribution.\n",
    "        nn.init.normal_(self.centroids, std=d_model ** -0.5)\n",
    "\n",
    "        # Register a buffer for the router bias. This bias is updated online\n",
    "        # without using standard gradient descent, hence it's not a parameter.\n",
    "        self.register_buffer(\"bias\", torch.zeros(n_routed_exp))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DeepSeekMoE layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], where B is\n",
    "                              batch size, S is sequence length, and D is d_model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape [B, S, D], which is the\n",
    "                          sum of the input, shared expert outputs, and routed\n",
    "                          expert outputs.\n",
    "        \"\"\"\n",
    "        # Get dimensions of the input tensor.\n",
    "        B, S, D = x.shape\n",
    "        # Reshape the input to [N, D], where N = B * S (number of tokens).\n",
    "        x_flat = x.reshape(-1, D)  # [N, D] with N=B*S\n",
    "\n",
    "        # 1) Shared path: Process the input through all shared experts and sum their outputs.\n",
    "        shared_out = torch.zeros_like(x)\n",
    "        for exp in self.shared:\n",
    "            shared_out += exp(x)\n",
    "        # (Optional) Scale the shared expert output by the number of shared experts.\n",
    "        # This can help in balancing the contribution of shared vs. routed experts.\n",
    "        # shared_out = shared_out / max(1, self.n_shared)\n",
    "\n",
    "        # 2) Router logits: Calculate the affinity of each token to each routed expert.\n",
    "        # Use autocasting to FP16 if fp16_router is True and the device is CUDA.\n",
    "        use_autocast = self.fp16_router and x.is_cuda\n",
    "        device_type = \"cuda\" if x.is_cuda else x.device.type\n",
    "        with torch.autocast(device_type=device_type, enabled=use_autocast):\n",
    "            # Calculate logits by taking the dot product of the flattened input with the expert centroids.\n",
    "            logits = F.linear(x_flat, self.centroids)  # [N, E]\n",
    "            # Add the router bias to the logits. Ensure bias matches the logits' dtype.\n",
    "            logits = logits + self.bias.to(logits.dtype)\n",
    "\n",
    "        # Select the top_k experts with the highest logits for each token.\n",
    "        topk_logits, topk_idx = torch.topk(logits, self.top_k, dim=-1)        # [N, k]\n",
    "        # Apply softmax to the top_k logits to get gating weights.\n",
    "        # Ensure the gate weights have the same dtype as the input for subsequent calculations.\n",
    "        gate = F.softmax(topk_logits, dim=-1, dtype=x.dtype)                   # [N, k]\n",
    "\n",
    "        # 3) Dispatch per expert: Route tokens to their selected experts and combine outputs.\n",
    "        routed_out = torch.zeros_like(x_flat)                                   # [N, D]\n",
    "        # Iterate through each routed expert.\n",
    "        for i in range(self.n_routed):\n",
    "            # Create a mask to identify which tokens selected the current expert (expert i).\n",
    "            mask = (topk_idx == i)\n",
    "            # Find the indices of the rows (tokens) and columns (which of the top-k) where expert i was selected.\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)                      # 1-D each\n",
    "            # If no tokens selected this expert, skip.\n",
    "            if row_idx.numel() == 0:\n",
    "                continue\n",
    "            # Select the input tokens that are routed to expert i.\n",
    "            exp_in = x_flat.index_select(0, row_idx)                            # [Ti, D] where Ti is the number of tokens routed to expert i\n",
    "            # Pass the selected tokens through the expert's FFN.\n",
    "            out = self.routed[i](exp_in)                                        # [Ti, D]\n",
    "            # Get the gating weights for the tokens routed to expert i.\n",
    "            w = gate[row_idx, which_k].unsqueeze(-1)                            # [Ti, 1]\n",
    "            # Scale the expert output by the gating weights and add it to the routed_out tensor\n",
    "            # at the original token positions using index_add_.\n",
    "            routed_out.index_add_(0, row_idx, out * w)\n",
    "\n",
    "        # Reshape the routed output back to the original [B, S, D] shape.\n",
    "        routed_out = routed_out.view(B, S, D)\n",
    "        # The final output is the sum of the original input, shared expert outputs, and routed expert outputs.\n",
    "        return x + shared_out + routed_out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_bias(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Updates the router bias based on expert load.\n",
    "\n",
    "        This method is typically called once per optimizer step using the\n",
    "        same batch of tokens that were passed through the forward method.\n",
    "        It uses the current router logits (including the current bias) to\n",
    "        estimate the load on each expert and adjusts the bias to encourage\n",
    "        a more balanced distribution of tokens across experts.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], identical\n",
    "                              to the input used in the corresponding forward pass.\n",
    "        \"\"\"\n",
    "        # Calculate the total number of tokens.\n",
    "        N = x.shape[0] * x.shape[1]\n",
    "        # Calculate the router logits (affinity scores) for each token and expert, including the current bias.\n",
    "        logits = F.linear(x.reshape(-1, self.d_model), self.centroids) + self.bias\n",
    "        # Determine the top_k experts selected for each token based on the current logits.\n",
    "        _, idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "\n",
    "        # Count how many times each expert was selected as one of the top_k.\n",
    "        counts = torch.bincount(idx.flatten(), minlength=self.n_routed).float()\n",
    "        # Calculate the average number of times an expert should ideally be selected.\n",
    "        avg = counts.sum() / max(1, self.n_routed)\n",
    "\n",
    "        # Calculate the \"violation\" for each expert. A positive violation means\n",
    "        # the expert is under-loaded compared to the average, and its bias\n",
    "        # should be increased to make it more likely to be selected in the future.\n",
    "        # A negative violation means it's over-loaded, and its bias should be decreased.\n",
    "        # Add a small epsilon (1e-6) to the denominator to avoid division by zero.\n",
    "        violation = (avg - counts) / (avg + 1e-6)\n",
    "        # Update the bias using a smooth, bounded update based on the violation.\n",
    "        # torch.tanh() squashes the violation into the range [-1, 1], preventing\n",
    "        # excessively large bias updates. The bias_lr controls the step size.\n",
    "        self.bias.add_(self.bias_lr * torch.tanh(violation))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c0a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New output shape: torch.Size([2, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the DeepSeekMoE class\n",
    "d_model = 1024\n",
    "n_routed_exp = 16\n",
    "n_shared_exp = 2\n",
    "top_k = 8\n",
    "\n",
    "model = DeepSeekMoE(d_model, n_routed_exp, n_shared_exp, top_k)\n",
    "\n",
    "# Create different random input data\n",
    "batch_size_new = 2\n",
    "seq_len_new = 64\n",
    "random_input_new = torch.randn(batch_size_new, seq_len_new, d_model)\n",
    "\n",
    "# Pass the new random input to the model's forward method\n",
    "output_new = model(random_input_new)\n",
    "\n",
    "print(\"New output shape:\", output_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e93705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
