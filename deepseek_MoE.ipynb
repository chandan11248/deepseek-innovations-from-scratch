{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09291127",
   "metadata": {},
   "source": [
    "# DeepSeek Mixture of Experts (MoE) - Simple Explanation\n",
    "\n",
    "## The Main Goal\n",
    "\n",
    "**Problem:** Large neural networks are expensive to run because every input goes through ALL parameters.\n",
    "\n",
    "**Solution:** Use **Mixture of Experts (MoE)** - have many small \"expert\" networks, but only use a FEW of them for each input!\n",
    "\n",
    "```\n",
    "Traditional Dense Network:          MoE Network:\n",
    "┌─────────────────────┐            ┌─────────────────────┐\n",
    "│  Every input uses   │            │  Each input only    │\n",
    "│  ALL parameters     │            │  uses SELECTED      │\n",
    "│  (expensive!)       │            │  experts (cheap!)   │\n",
    "└─────────────────────┘            └─────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "                         Input Token\n",
    "                              │\n",
    "                              ▼\n",
    "              ┌───────────────┴───────────────┐\n",
    "              │                               │\n",
    "              ▼                               ▼\n",
    "      ┌──────────────┐                ┌──────────────┐\n",
    "      │    SHARED    │                │    ROUTER    │\n",
    "      │   EXPERTS    │                │ (Gatekeeper) │\n",
    "      │ (Always ON)  │                └──────┬───────┘\n",
    "      └──────┬───────┘                       │\n",
    "             │                      Picks Top-K Experts\n",
    "             │                               │\n",
    "             │                    ┌──────────┼──────────┐\n",
    "             │                    ▼          ▼          ▼\n",
    "             │               ┌────────┐  ┌────────┐  ┌────────┐\n",
    "             │               │Expert 1│  │Expert 5│  │Expert 9│  <- Only these\n",
    "             │               │        │  │        │  │        │     activated!\n",
    "             │               └────┬───┘  └────┬───┘  └────┬───┘\n",
    "             │                    │          │          │\n",
    "             │                    └──────────┼──────────┘\n",
    "             │                               │\n",
    "             │                     Weighted Sum (gating)\n",
    "             │                               │\n",
    "             └───────────────┬───────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                       ┌───────────┐\n",
    "                       │    ADD    │  <- Combine all outputs\n",
    "                       │ (x+s+r)   │\n",
    "                       └─────┬─────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                        Final Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components Explained\n",
    "\n",
    "### 1. Expert FFN (Feed-Forward Network)\n",
    "\n",
    "Each expert is a **small 2-layer neural network**:\n",
    "\n",
    "```\n",
    "Input ──▶ [Linear Layer 1] ──▶ [GELU Activation] ──▶ [Linear Layer 2] ──▶ Output\n",
    "         (expand)              (non-linearity)       (compress back)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Simple: Input → Hidden → Output\n",
    "fc1: d_model → hidden    # Expand dimensions\n",
    "GELU: activation         # Add non-linearity  \n",
    "fc2: hidden → d_model    # Back to original size\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Shared Experts (Always Active)\n",
    "\n",
    "These experts process **EVERY token** - they're always \"on duty\":\n",
    "\n",
    "```\n",
    "Token 1 ──▶ ┌─────────────────┐\n",
    "Token 2 ──▶ │  Shared Expert  │ ──▶ ALL tokens get processed\n",
    "Token 3 ──▶ │    (Always ON)  │\n",
    "   ...  ──▶ └─────────────────┘\n",
    "```\n",
    "\n",
    "**Why?** They capture general patterns that apply to all inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Routed Experts (Selectively Active)\n",
    "\n",
    "The **Router** decides which experts to use for each token:\n",
    "\n",
    "```\n",
    "          16 Routed Experts Available\n",
    "    ┌───┬───┬───┬───┬───┬───┬───┬───┐\n",
    "    │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │ ...\n",
    "    └───┴───┴───┴───┴───┴───┴───┴───┘\n",
    "          │       │   │           │\n",
    "          ▼       ▼   ▼           ▼\n",
    "        Token A picks: Expert 2, 4, 5, 8 (top_k = 4)\n",
    "        \n",
    "    ┌───┬───┬───┬───┬───┬───┬───┬───┐\n",
    "    │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │ ...\n",
    "    └───┴───┴───┴───┴───┴───┴───┴───┘\n",
    "      │           │       │   │\n",
    "      ▼           ▼       ▼   ▼\n",
    "    Token B picks: Expert 1, 4, 6, 7 (different experts!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. The Router (Gatekeeper)\n",
    "\n",
    "The router calculates **affinity scores** between each token and each expert:\n",
    "\n",
    "```\n",
    "                    How Router Works\n",
    "                    ════════════════\n",
    "                    \n",
    "Token Embedding ──▶ Dot Product with Expert Centroids ──▶ Scores\n",
    "     [1024]              [16 × 1024]                      [16]\n",
    "                    \n",
    "     \"How similar is this token to each expert's specialty?\"\n",
    "```\n",
    "\n",
    "```\n",
    "Score Calculation:\n",
    "┌────────────────────────────────────────────┐\n",
    "│  logits = token · centroids + bias         │\n",
    "│                                            │\n",
    "│  Token: \"The cat sat...\"                   │\n",
    "│                                            │\n",
    "│  Expert 1 (grammar):     score = 0.8       │\n",
    "│  Expert 2 (math):        score = 0.1       │\n",
    "│  Expert 3 (code):        score = 0.2       │\n",
    "│  Expert 4 (language):    score = 0.9       │\n",
    "│  ...                                       │\n",
    "│                                            │\n",
    "│  → Select TOP-K highest scores!            │\n",
    "└────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Gating Weights (Softmax)\n",
    "\n",
    "After selecting top-k experts, we **weight their contributions**:\n",
    "\n",
    "```\n",
    "Selected Experts:    Expert 4    Expert 1    Expert 7    Expert 2\n",
    "Top-k Logits:          0.9         0.8         0.6         0.5\n",
    "                        │           │           │           │\n",
    "                        ▼           ▼           ▼           ▼\n",
    "                   ┌─────────── Softmax ───────────┐\n",
    "                        │           │           │           │\n",
    "                        ▼           ▼           ▼           ▼\n",
    "Gate Weights:         0.35        0.30        0.20        0.15\n",
    "                        │           │           │           │\n",
    "                        │     (Higher score = More influence)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Pass Flow\n",
    "\n",
    "```\n",
    "Step 1: Input arrives\n",
    "        ┌─────────────────────────┐\n",
    "        │  x: [Batch, Seq, 1024]  │\n",
    "        └───────────┬─────────────┘\n",
    "                    │\n",
    "Step 2: Shared experts process ALL tokens\n",
    "        ┌───────────▼─────────────┐\n",
    "        │  shared_out = Σ exp(x)  │\n",
    "        └───────────┬─────────────┘\n",
    "                    │\n",
    "Step 3: Router calculates expert preferences\n",
    "        ┌───────────▼─────────────┐\n",
    "        │  logits = x · centroids │\n",
    "        │  + bias                 │\n",
    "        └───────────┬─────────────┘\n",
    "                    │\n",
    "Step 4: Select top-k experts per token\n",
    "        ┌───────────▼─────────────┐\n",
    "        │  top_k = 8 experts      │\n",
    "        │  (out of 16 available)  │\n",
    "        └───────────┬─────────────┘\n",
    "                    │\n",
    "Step 5: Route tokens to selected experts\n",
    "        ┌───────────▼─────────────┐\n",
    "        │  Each expert processes  │\n",
    "        │  only ITS tokens        │\n",
    "        └───────────┬─────────────┘\n",
    "                    │\n",
    "Step 6: Weight and combine outputs\n",
    "        ┌───────────▼─────────────┐\n",
    "        │  routed_out = Σ(w * out)│\n",
    "        └───────────┬─────────────┘\n",
    "                    │\n",
    "Step 7: Final output = input + shared + routed\n",
    "        ┌───────────▼─────────────┐\n",
    "        │  return x + shared_out  │\n",
    "        │         + routed_out    │\n",
    "        └─────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Load Balancing (Bias Update)\n",
    "\n",
    "**Problem:** Some experts might get ALL the tokens (overloaded), while others get NONE (underused).\n",
    "\n",
    "**Solution:** Dynamically adjust the router bias!\n",
    "\n",
    "```\n",
    "                   Expert Load Distribution\n",
    "    \n",
    "    Before Balancing:                   After Balancing:\n",
    "    \n",
    "    Expert 1: ████████████ (120)        Expert 1: ████████ (80)\n",
    "    Expert 2: ██           (20)         Expert 2: ███████  (70)\n",
    "    Expert 3: █            (10)         Expert 3: ████████ (75)\n",
    "    Expert 4: ████████████████ (160)    Expert 4: ███████  (75)\n",
    "    \n",
    "    Unbalanced!                         Balanced!\n",
    "```\n",
    "\n",
    "```\n",
    "How Bias Update Works:\n",
    "══════════════════════\n",
    "\n",
    "1. Count how many tokens each expert received\n",
    "2. Calculate average load\n",
    "3. If expert is UNDER-loaded → INCREASE its bias (make it more attractive)\n",
    "4. If expert is OVER-loaded → DECREASE its bias (make it less attractive)\n",
    "\n",
    "Formula: bias += lr * tanh((avg - count) / avg)\n",
    "         └────────────────────────────────────┘\n",
    "                 Smooth, bounded update\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example with Numbers\n",
    "\n",
    "```python\n",
    "# Configuration\n",
    "d_model = 1024        # Token dimension\n",
    "n_routed_exp = 16     # 16 routed experts available  \n",
    "n_shared_exp = 2      # 2 shared experts (always active)\n",
    "top_k = 8             # Select 8 experts per token\n",
    "\n",
    "# Input\n",
    "batch_size = 2\n",
    "seq_len = 64\n",
    "# Total tokens = 2 × 64 = 128 tokens\n",
    "```\n",
    "\n",
    "```\n",
    "For each of 128 tokens:\n",
    "├── 2 shared experts process it (ALWAYS)\n",
    "└── Router selects 8 out of 16 routed experts\n",
    "    └── These 8 experts process it with weighted contributions\n",
    "\n",
    "Computation Savings:\n",
    "├── Dense model: ALL parameters for ALL tokens\n",
    "└── MoE model: Only 8/16 = 50% of routed experts per token!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Summary\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                       DeepSeek MoE Layer                        │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│   INPUT ──────────┬─────────────────────┐                       │\n",
    "│                   │                     │                       │\n",
    "│                   ▼                     ▼                       │\n",
    "│            ┌──────────────┐      ┌─────────────┐                │\n",
    "│            │   SHARED     │      │   ROUTER    │                │\n",
    "│            │   EXPERTS    │      │  + GATING   │                │\n",
    "│            │  (2 always)  │      └──────┬──────┘                │\n",
    "│            └──────┬───────┘             │                       │\n",
    "│                   │              ┌──────┴──────┐                │\n",
    "│                   │              │  TOP-K = 8  │                │\n",
    "│                   │              │  Selection  │                │\n",
    "│                   │              └──────┬──────┘                │\n",
    "│                   │                     │                       │\n",
    "│                   │              ┌──────┴──────┐                │\n",
    "│                   │              │  8 ROUTED   │                │\n",
    "│                   │              │  EXPERTS    │                │\n",
    "│                   │              │  (selected) │                │\n",
    "│                   │              └──────┬──────┘                │\n",
    "│                   │                     │                       │\n",
    "│                   └─────────┬───────────┘                       │\n",
    "│                             │                                   │\n",
    "│                       ┌─────┴─────┐                             │\n",
    "│                       │    ADD    │                             │\n",
    "│                       │  (x+s+r)  │                             │\n",
    "│                       └─────┬─────┘                             │\n",
    "│                             │                                   │\n",
    "│                          OUTPUT                                 │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | What It Does |\n",
    "|---------|--------------|\n",
    "| **Shared Experts** | Always active, capture common patterns |\n",
    "| **Routed Experts** | Specialized, only activated when needed |\n",
    "| **Router** | Decides which experts to use per token |\n",
    "| **Top-K Selection** | Limits computation by selecting few experts |\n",
    "| **Gating** | Weights expert outputs by relevance |\n",
    "| **Bias Update** | Keeps expert loads balanced |\n",
    "| **Residual Connection** | Output = Input + Shared + Routed |\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "```\n",
    "Traditional Transformer FFN:           vs        MoE FFN:\n",
    "──────────────────────────                       ─────────\n",
    "All 256 experts active                           Only 8 experts active\n",
    "= 256× computation                               = 8× computation\n",
    "= SLOW + EXPENSIVE                               = FAST + CHEAP!\n",
    "\n",
    "Same model capacity, much less compute!\n",
    "```\n",
    "\n",
    "This is how DeepSeek-V3 achieves **large model capacity** with **efficient inference**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adf2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math \n",
    "from contextlib import nullcontext \n",
    "from typing import Optional \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0429ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Slightly faster GELU (approx)\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) *\n",
    "                                       (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class ExpertFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer MLP expert. Hidden dim is usually smaller than a dense FFN\n",
    "    (e.g., 0.25 × d_model in DeepSeek-V3).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, hidden: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.dropout(_gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSeek-V3 style Mixture-of-Experts (MoE) layer.\n",
    "\n",
    "    This MoE layer incorporates both routed experts (selected by a router)\n",
    "    and shared experts (applied to all inputs). It is designed based on\n",
    "    the architecture described in the DeepSeek-V3 paper.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output features.\n",
    "        n_routed_exp (int): The number of routed experts.\n",
    "        n_shared_exp (int, optional): The number of shared experts. Defaults to 1.\n",
    "        top_k (int, optional): The number of routed experts to select for each token.\n",
    "                               Defaults to 8.\n",
    "        routed_hidden (int, optional): The hidden dimension for routed experts.\n",
    "                                      Defaults to 2048.\n",
    "        shared_hidden (Optional[int], optional): The hidden dimension for shared experts.\n",
    "                                                If None, uses routed_hidden. Defaults to None.\n",
    "        bias_lr (float, optional): Learning rate for the router bias (updated online).\n",
    "                                   Defaults to 0.01.\n",
    "        fp16_router (bool, optional): Whether to use FP16 precision for router calculations.\n",
    "                                     Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_routed_exp: int,\n",
    "        n_shared_exp: int = 1,\n",
    "        top_k: int = 8,\n",
    "        routed_hidden: int = 2_048,\n",
    "        shared_hidden: Optional[int] = None,\n",
    "        bias_lr: float = 0.01,\n",
    "        fp16_router: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Assert that the number of selected experts (top_k) is less than or equal to the total number of routed experts.\n",
    "        assert top_k <= n_routed_exp, \"k must be ≤ number of routed experts\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_routed = n_routed_exp\n",
    "        self.n_shared = n_shared_exp\n",
    "        self.top_k = top_k\n",
    "        self.bias_lr = bias_lr\n",
    "        self.fp16_router = fp16_router\n",
    "\n",
    "        # Module list for the routed experts.\n",
    "        self.routed = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "        )\n",
    "        # Determine the hidden dimension for shared experts. Use routed_hidden if shared_hidden is not provided.\n",
    "        hidden_shared = shared_hidden or routed_hidden\n",
    "        # Module list for the shared experts.\n",
    "        self.shared = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, hidden_shared) for _ in range(n_shared_exp)]\n",
    "        )\n",
    "\n",
    "        # Register a parameter for the centroids used by the router.\n",
    "        # Centroids represent the \"preference\" of each expert for different input features.\n",
    "        self.register_parameter(\"centroids\", nn.Parameter(torch.empty(n_routed_exp, d_model)))\n",
    "        # Initialize centroids with a normal distribution.\n",
    "        nn.init.normal_(self.centroids, std=d_model ** -0.5)\n",
    "\n",
    "        # Register a buffer for the router bias. This bias is updated online\n",
    "        # without using standard gradient descent, hence it's not a parameter.\n",
    "        self.register_buffer(\"bias\", torch.zeros(n_routed_exp))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DeepSeekMoE layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], where B is\n",
    "                              batch size, S is sequence length, and D is d_model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape [B, S, D], which is the\n",
    "                          sum of the input, shared expert outputs, and routed\n",
    "                          expert outputs.\n",
    "        \"\"\"\n",
    "        # Get dimensions of the input tensor.\n",
    "        B, S, D = x.shape\n",
    "        # Reshape the input to [N, D], where N = B * S (number of tokens).\n",
    "        x_flat = x.reshape(-1, D)  # [N, D] with N=B*S\n",
    "\n",
    "        # 1) Shared path: Process the input through all shared experts and sum their outputs.\n",
    "        shared_out = torch.zeros_like(x)\n",
    "        for exp in self.shared:\n",
    "            shared_out += exp(x)\n",
    "        # (Optional) Scale the shared expert output by the number of shared experts.\n",
    "        # This can help in balancing the contribution of shared vs. routed experts.\n",
    "        # shared_out = shared_out / max(1, self.n_shared)\n",
    "\n",
    "        # 2) Router logits: Calculate the affinity of each token to each routed expert.\n",
    "        # Use autocasting to FP16 if fp16_router is True and the device is CUDA.\n",
    "        use_autocast = self.fp16_router and x.is_cuda\n",
    "        device_type = \"cuda\" if x.is_cuda else x.device.type\n",
    "        with torch.autocast(device_type=device_type, enabled=use_autocast):\n",
    "            # Calculate logits by taking the dot product of the flattened input with the expert centroids.\n",
    "            logits = F.linear(x_flat, self.centroids)  # [N, E]\n",
    "            # Add the router bias to the logits. Ensure bias matches the logits' dtype.\n",
    "            logits = logits + self.bias.to(logits.dtype)\n",
    "\n",
    "        # Select the top_k experts with the highest logits for each token.\n",
    "        topk_logits, topk_idx = torch.topk(logits, self.top_k, dim=-1)        # [N, k]\n",
    "        # Apply softmax to the top_k logits to get gating weights.\n",
    "        # Ensure the gate weights have the same dtype as the input for subsequent calculations.\n",
    "        gate = F.softmax(topk_logits, dim=-1, dtype=x.dtype)                   # [N, k]\n",
    "\n",
    "        # 3) Dispatch per expert: Route tokens to their selected experts and combine outputs.\n",
    "        routed_out = torch.zeros_like(x_flat)                                   # [N, D]\n",
    "        # Iterate through each routed expert.\n",
    "        for i in range(self.n_routed):\n",
    "            # Create a mask to identify which tokens selected the current expert (expert i).\n",
    "            mask = (topk_idx == i)\n",
    "            # Find the indices of the rows (tokens) and columns (which of the top-k) where expert i was selected.\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)                      # 1-D each\n",
    "            # If no tokens selected this expert, skip.\n",
    "            if row_idx.numel() == 0:\n",
    "                continue\n",
    "            # Select the input tokens that are routed to expert i.\n",
    "            exp_in = x_flat.index_select(0, row_idx)                            # [Ti, D] where Ti is the number of tokens routed to expert i\n",
    "            # Pass the selected tokens through the expert's FFN.\n",
    "            out = self.routed[i](exp_in)                                        # [Ti, D]\n",
    "            # Get the gating weights for the tokens routed to expert i.\n",
    "            w = gate[row_idx, which_k].unsqueeze(-1)                            # [Ti, 1]\n",
    "            # Scale the expert output by the gating weights and add it to the routed_out tensor\n",
    "            # at the original token positions using index_add_.\n",
    "            routed_out.index_add_(0, row_idx, out * w)\n",
    "\n",
    "        # Reshape the routed output back to the original [B, S, D] shape.\n",
    "        routed_out = routed_out.view(B, S, D)\n",
    "        # The final output is the sum of the original input, shared expert outputs, and routed expert outputs.\n",
    "        return x + shared_out + routed_out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_bias(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Updates the router bias based on expert load.\n",
    "\n",
    "        This method is typically called once per optimizer step using the\n",
    "        same batch of tokens that were passed through the forward method.\n",
    "        It uses the current router logits (including the current bias) to\n",
    "        estimate the load on each expert and adjusts the bias to encourage\n",
    "        a more balanced distribution of tokens across experts.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], identical\n",
    "                              to the input used in the corresponding forward pass.\n",
    "        \"\"\"\n",
    "        # Calculate the total number of tokens.\n",
    "        N = x.shape[0] * x.shape[1]\n",
    "        # Calculate the router logits (affinity scores) for each token and expert, including the current bias.\n",
    "        logits = F.linear(x.reshape(-1, self.d_model), self.centroids) + self.bias\n",
    "        # Determine the top_k experts selected for each token based on the current logits.\n",
    "        _, idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "\n",
    "        # Count how many times each expert was selected as one of the top_k.\n",
    "        counts = torch.bincount(idx.flatten(), minlength=self.n_routed).float()\n",
    "        # Calculate the average number of times an expert should ideally be selected.\n",
    "        avg = counts.sum() / max(1, self.n_routed)\n",
    "\n",
    "        # Calculate the \"violation\" for each expert. A positive violation means\n",
    "        # the expert is under-loaded compared to the average, and its bias\n",
    "        # should be increased to make it more likely to be selected in the future.\n",
    "        # A negative violation means it's over-loaded, and its bias should be decreased.\n",
    "        # Add a small epsilon (1e-6) to the denominator to avoid division by zero.\n",
    "        violation = (avg - counts) / (avg + 1e-6)\n",
    "        # Update the bias using a smooth, bounded update based on the violation.\n",
    "        # torch.tanh() squashes the violation into the range [-1, 1], preventing\n",
    "        # excessively large bias updates. The bias_lr controls the step size.\n",
    "        self.bias.add_(self.bias_lr * torch.tanh(violation))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c0a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New output shape: torch.Size([2, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the DeepSeekMoE class\n",
    "d_model = 1024\n",
    "n_routed_exp = 16\n",
    "n_shared_exp = 2\n",
    "top_k = 8\n",
    "\n",
    "model = DeepSeekMoE(d_model, n_routed_exp, n_shared_exp, top_k)\n",
    "\n",
    "# Create different random input data\n",
    "batch_size_new = 2\n",
    "seq_len_new = 64\n",
    "random_input_new = torch.randn(batch_size_new, seq_len_new, d_model)\n",
    "\n",
    "# Pass the new random input to the model's forward method\n",
    "output_new = model(random_input_new)\n",
    "\n",
    "print(\"New output shape:\", output_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e93705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56974d16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1e7d496",
   "metadata": {},
   "source": [
    "# DeepSeek MoE - Code Walkthrough\n",
    "\n",
    "This document explains **every line of code** in simple terms.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Imports\n",
    "\n",
    "```python\n",
    "import math \n",
    "from contextlib import nullcontext \n",
    "from typing import Optional \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "```\n",
    "\n",
    "| Import | What It Does |\n",
    "|--------|--------------|\n",
    "| `math` | Python's math library (for `sqrt`, `pi`, etc.) |\n",
    "| `nullcontext` | A \"do nothing\" context manager (placeholder) |\n",
    "| `Optional` | Type hint meaning \"this can be None\" |\n",
    "| `torch` | PyTorch - the deep learning framework |\n",
    "| `torch.nn` | Neural network building blocks (layers, modules) |\n",
    "| `torch.nn.functional as F` | Functional operations (softmax, linear, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. GELU Activation Function\n",
    "\n",
    "```python\n",
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) *\n",
    "                                       (x + 0.044715 * torch.pow(x, 3))))\n",
    "```\n",
    "\n",
    "### What is GELU?\n",
    "GELU = **G**aussian **E**rror **L**inear **U**nit\n",
    "\n",
    "It's a smooth activation function (like ReLU but smoother):\n",
    "\n",
    "```\n",
    "Input x:     -2    -1     0     1     2\n",
    "GELU(x):   -0.04  -0.16   0    0.84  1.95\n",
    "\n",
    "         ReLU:              GELU:\n",
    "         │    /             │    ╱\n",
    "         │   /              │  ╱\n",
    "    ─────┼──/────      ────╱┼─────\n",
    "         │                ╱  │\n",
    "         │              ╱    │\n",
    "```\n",
    "\n",
    "### Code Breakdown:\n",
    "\n",
    "```python\n",
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "#   └─────┘               └───────────────┘\n",
    "#   function name         returns a tensor\n",
    "#          └───────────────┘\n",
    "#          x is a tensor (type hint)\n",
    "```\n",
    "\n",
    "```python\n",
    "return 0.5 * x * (1.0 + torch.tanh(...))\n",
    "#      └───────────────────────────────┘\n",
    "#      This is the approximate GELU formula\n",
    "#      (faster than exact GELU)\n",
    "```\n",
    "\n",
    "```python\n",
    "math.sqrt(2.0 / math.pi)  # ≈ 0.7979 (a constant)\n",
    "x + 0.044715 * torch.pow(x, 3)  # x + 0.044715 * x³\n",
    "#                 └───────────┘\n",
    "#                 x raised to power 3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ExpertFFN Class (Feed-Forward Network)\n",
    "\n",
    "```python\n",
    "class ExpertFFN(nn.Module):\n",
    "```\n",
    "- `class ExpertFFN` → Define a new class called ExpertFFN\n",
    "- `nn.Module` → Inherit from PyTorch's base neural network class\n",
    "\n",
    "### Constructor (`__init__`)\n",
    "\n",
    "```python\n",
    "def __init__(self, d_model: int, hidden: int, dropout: float = 0.0):\n",
    "#            └──┘  └───────────┘ └──────────┘ └─────────────────┘\n",
    "#            self   input dim    hidden dim   dropout rate (default 0)\n",
    "    \n",
    "    super().__init__()\n",
    "#   └────────────────┘\n",
    "#   Call parent class constructor (required for nn.Module)\n",
    "```\n",
    "\n",
    "```python\n",
    "    self.fc1 = nn.Linear(d_model, hidden, bias=False)\n",
    "#   └──────┘   └───────────────────────────────────┘\n",
    "#   save as    Create a linear layer: d_model → hidden\n",
    "#   attribute  bias=False means no bias term (just weights)\n",
    "```\n",
    "\n",
    "```\n",
    "Linear Layer Visualization:\n",
    "                    \n",
    "  Input [d_model]          Weights [d_model × hidden]         Output [hidden]\n",
    "  ┌───┐                    ┌─────────────────────┐            ┌───┐\n",
    "  │ x₁│ ──────────────────▶│  W₁₁  W₁₂  ...  W₁ₕ │──────────▶│ y₁│\n",
    "  │ x₂│                    │  W₂₁  W₂₂  ...  W₂ₕ │           │ y₂│\n",
    "  │...│                    │  ...  ...  ...  ... │           │...│\n",
    "  │ xₙ│                    │  Wₙ₁  Wₙ₂  ...  Wₙₕ │           │ yₕ│\n",
    "  └───┘                    └─────────────────────┘            └───┘\n",
    "  \n",
    "  Formula: y = x @ W  (matrix multiplication)\n",
    "```\n",
    "\n",
    "```python\n",
    "    self.fc2 = nn.Linear(hidden, d_model, bias=False)\n",
    "#   Second linear layer: hidden → d_model (back to original size)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "#   Dropout layer: randomly zeros some neurons during training\n",
    "#   (helps prevent overfitting)\n",
    "```\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "```python\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.fc2(self.dropout(_gelu(self.fc1(x))))\n",
    "#          └─────────────────────────────────────────┘\n",
    "#          Chain of operations (read inside-out):\n",
    "```\n",
    "\n",
    "```\n",
    "Step-by-step:\n",
    "┌───────────────────────────────────────────────────────────────┐\n",
    "│  1. self.fc1(x)           →  x goes through first linear     │\n",
    "│  2. _gelu(...)            →  apply GELU activation           │\n",
    "│  3. self.dropout(...)     →  randomly drop some values       │\n",
    "│  4. self.fc2(...)         →  second linear layer (output)    │\n",
    "└───────────────────────────────────────────────────────────────┘\n",
    "\n",
    "x [d_model] → fc1 → [hidden] → GELU → dropout → fc2 → [d_model]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. DeepSeekMoE Class - Constructor\n",
    "\n",
    "```python\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,           # Dimension of input/output (e.g., 1024)\n",
    "        n_routed_exp: int,      # Number of routed experts (e.g., 16)\n",
    "        n_shared_exp: int = 1,  # Number of shared experts (default 1)\n",
    "        top_k: int = 8,         # How many experts to select per token\n",
    "        routed_hidden: int = 2_048,  # Hidden dim for routed experts\n",
    "        shared_hidden: Optional[int] = None,  # Hidden dim for shared (or None)\n",
    "        bias_lr: float = 0.01,  # Learning rate for bias update\n",
    "        fp16_router: bool = False,  # Use FP16 for router? (faster on GPU)\n",
    "    ):\n",
    "```\n",
    "\n",
    "### Assertions and Attributes\n",
    "\n",
    "```python\n",
    "        super().__init__()\n",
    "        \n",
    "        assert top_k <= n_routed_exp, \"k must be ≤ number of routed experts\"\n",
    "#       └────┘ └──────────────────┘  └─────────────────────────────────────┘\n",
    "#       check  condition to check    error message if condition is False\n",
    "```\n",
    "\n",
    "```python\n",
    "        # Store all parameters as instance attributes\n",
    "        self.d_model = d_model        # Save for later use\n",
    "        self.n_routed = n_routed_exp  # Number of routed experts\n",
    "        self.n_shared = n_shared_exp  # Number of shared experts\n",
    "        self.top_k = top_k            # K in \"top-k\" selection\n",
    "        self.bias_lr = bias_lr        # Bias learning rate\n",
    "        self.fp16_router = fp16_router  # FP16 flag\n",
    "```\n",
    "\n",
    "### Creating Expert Networks\n",
    "\n",
    "```python\n",
    "        self.routed = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "        )\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "```python\n",
    "nn.ModuleList([...])\n",
    "#   └──────────────┘\n",
    "#   A list of nn.Modules that PyTorch can track\n",
    "#   (regular Python list won't work for gradient tracking!)\n",
    "```\n",
    "\n",
    "```python\n",
    "[ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "#└────────────────────────────────────────────────────────────┘\n",
    "#         List comprehension: create n_routed_exp experts\n",
    "```\n",
    "\n",
    "```\n",
    "Visual:\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  self.routed = [Expert₀, Expert₁, Expert₂, ... Expert₁₅]   │\n",
    "│                    │        │        │           │          │\n",
    "│                  FFN      FFN      FFN         FFN          │\n",
    "│                (1024→   (1024→   (1024→      (1024→         │\n",
    "│                 2048→    2048→    2048→       2048→         │\n",
    "│                 1024)    1024)    1024)       1024)         │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "```python\n",
    "        hidden_shared = shared_hidden or routed_hidden\n",
    "#       └───────────────────────────────────────────┘\n",
    "#       If shared_hidden is None, use routed_hidden instead\n",
    "#       (Python's \"or\" returns first truthy value)\n",
    "        \n",
    "        self.shared = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, hidden_shared) for _ in range(n_shared_exp)]\n",
    "        )\n",
    "#       Same pattern: create n_shared_exp shared experts\n",
    "```\n",
    "\n",
    "### Router Components\n",
    "\n",
    "```python\n",
    "        self.register_parameter(\"centroids\", \n",
    "                                nn.Parameter(torch.empty(n_routed_exp, d_model)))\n",
    "```\n",
    "\n",
    "**What is this?**\n",
    "\n",
    "```python\n",
    "register_parameter(\"name\", parameter)\n",
    "#   └──────────────────────────────────┘\n",
    "#   Register a learnable parameter with PyTorch\n",
    "#   (will be updated during training via gradients)\n",
    "```\n",
    "\n",
    "```python\n",
    "torch.empty(n_routed_exp, d_model)\n",
    "#   └────────────────────────────┘\n",
    "#   Create uninitialized tensor of shape [16, 1024]\n",
    "#   (will be initialized next)\n",
    "```\n",
    "\n",
    "```\n",
    "Centroids shape: [n_routed_exp, d_model] = [16, 1024]\n",
    "\n",
    "Each row = one expert's \"centroid\" (preference vector)\n",
    "\n",
    "         d_model = 1024 dimensions\n",
    "         ←──────────────────────────→\n",
    "    ┌────────────────────────────────┐  ↑\n",
    "    │  Expert 0's centroid vector    │  │\n",
    "    ├────────────────────────────────┤  │\n",
    "    │  Expert 1's centroid vector    │  │ n_routed_exp\n",
    "    ├────────────────────────────────┤  │ = 16 experts\n",
    "    │           ...                  │  │\n",
    "    ├────────────────────────────────┤  │\n",
    "    │  Expert 15's centroid vector   │  ↓\n",
    "    └────────────────────────────────┘\n",
    "```\n",
    "\n",
    "```python\n",
    "        nn.init.normal_(self.centroids, std=d_model ** -0.5)\n",
    "#       └───────────────────────────────────────────────────┘\n",
    "#       Initialize with normal distribution\n",
    "#       std = 1/√1024 ≈ 0.031 (small values)\n",
    "```\n",
    "\n",
    "```python\n",
    "        self.register_buffer(\"bias\", torch.zeros(n_routed_exp))\n",
    "#       └──────────────────────────────────────────────────────┘\n",
    "#       register_buffer = NOT a learnable parameter\n",
    "#       (won't get gradients, but will be saved with model)\n",
    "#       \n",
    "#       torch.zeros(16) = [0, 0, 0, ... 0] (16 zeros)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Forward Pass - Step by Step\n",
    "\n",
    "```python\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "```\n",
    "\n",
    "### Step 1: Get Dimensions\n",
    "\n",
    "```python\n",
    "        B, S, D = x.shape\n",
    "#       └─────────────────┘\n",
    "#       Unpack shape: Batch size, Sequence length, Dimension\n",
    "#       e.g., x.shape = [2, 64, 1024]\n",
    "#             B=2, S=64, D=1024\n",
    "```\n",
    "\n",
    "```python\n",
    "        x_flat = x.reshape(-1, D)  # [N, D] with N=B*S\n",
    "#               └────────────────┘\n",
    "#       Flatten first two dimensions\n",
    "#       [2, 64, 1024] → [128, 1024]\n",
    "#       \n",
    "#       -1 means \"calculate this dimension automatically\"\n",
    "```\n",
    "\n",
    "```\n",
    "Before:                          After:\n",
    "x: [B, S, D]                     x_flat: [N, D]\n",
    "   [2, 64, 1024]                         [128, 1024]\n",
    "   \n",
    "   ┌─────────────┐               ┌─────────────┐\n",
    "   │  Batch 0    │               │  Token 0    │\n",
    "   │ ┌─────────┐ │               │  Token 1    │\n",
    "   │ │Token 0-63│ │   flatten    │  Token 2    │\n",
    "   │ └─────────┘ │   ───────▶    │    ...      │\n",
    "   ├─────────────┤               │  Token 127  │\n",
    "   │  Batch 1    │               └─────────────┘\n",
    "   │ ┌─────────┐ │               \n",
    "   │ │Token 0-63│ │               N = 2×64 = 128\n",
    "   │ └─────────┘ │               \n",
    "   └─────────────┘               \n",
    "```\n",
    "\n",
    "### Step 2: Shared Experts Path\n",
    "\n",
    "```python\n",
    "        shared_out = torch.zeros_like(x)\n",
    "#       └──────────────────────────────┘\n",
    "#       Create zero tensor with same shape as x: [B, S, D]\n",
    "```\n",
    "\n",
    "```python\n",
    "        for exp in self.shared:\n",
    "            shared_out += exp(x)\n",
    "#       └────────────────────────┘\n",
    "#       Loop through each shared expert\n",
    "#       Add each expert's output to shared_out\n",
    "```\n",
    "\n",
    "```\n",
    "Visual:\n",
    "        x ──────┬──────▶ Shared Expert 0 ──▶ output₀\n",
    "                │\n",
    "                └──────▶ Shared Expert 1 ──▶ output₁\n",
    "                \n",
    "        shared_out = output₀ + output₁\n",
    "```\n",
    "\n",
    "### Step 3: Router Logits\n",
    "\n",
    "```python\n",
    "        use_autocast = self.fp16_router and x.is_cuda\n",
    "#       └────────────────────────────────────────────┘\n",
    "#       Use FP16 only if flag is True AND on GPU\n",
    "        \n",
    "        device_type = \"cuda\" if x.is_cuda else x.device.type\n",
    "#       └───────────────────────────────────────────────────┘\n",
    "#       Get device type string for autocast\n",
    "```\n",
    "\n",
    "```python\n",
    "        with torch.autocast(device_type=device_type, enabled=use_autocast):\n",
    "#       └──────────────────────────────────────────────────────────────────┘\n",
    "#       Context manager for mixed precision (FP16)\n",
    "#       Makes computation faster on modern GPUs\n",
    "```\n",
    "\n",
    "```python\n",
    "            logits = F.linear(x_flat, self.centroids)  # [N, E]\n",
    "#                   └──────────────────────────────────┘\n",
    "#           Compute dot product between tokens and expert centroids\n",
    "#           \n",
    "#           x_flat:    [N, D] = [128, 1024]\n",
    "#           centroids: [E, D] = [16, 1024]\n",
    "#           logits:    [N, E] = [128, 16]\n",
    "```\n",
    "\n",
    "```\n",
    "F.linear(input, weight) = input @ weight.T\n",
    "\n",
    "For each token, compute similarity with each expert:\n",
    "\n",
    "Token₀ • Centroid₀ = score₀₀   ─┐\n",
    "Token₀ • Centroid₁ = score₀₁    │ ← logits for Token 0\n",
    "Token₀ • Centroid₂ = score₀₂    │\n",
    "...                             ─┘\n",
    "\n",
    "Result: [128 tokens × 16 experts] = 128×16 scores\n",
    "```\n",
    "\n",
    "```python\n",
    "            logits = logits + self.bias.to(logits.dtype)\n",
    "#                   └──────────────────────────────────┘\n",
    "#           Add bias to each expert's score\n",
    "#           .to(logits.dtype) ensures matching data types\n",
    "```\n",
    "\n",
    "### Step 4: Top-K Selection\n",
    "\n",
    "```python\n",
    "        topk_logits, topk_idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "#       └─────────────────────────────────────────────────────────────┘\n",
    "#       Select top_k highest values along last dimension\n",
    "#       \n",
    "#       topk_logits: [N, k] = [128, 8] ← the actual scores\n",
    "#       topk_idx:    [N, k] = [128, 8] ← which experts were selected\n",
    "```\n",
    "\n",
    "```\n",
    "Example for one token:\n",
    "logits = [0.1, 0.9, 0.3, 0.7, 0.2, 0.8, 0.4, 0.6, ...]  (16 values)\n",
    "                ↑        ↑        ↑\n",
    "         torch.topk(logits, k=3) selects top 3:\n",
    "         \n",
    "topk_logits = [0.9, 0.8, 0.7]  ← highest scores\n",
    "topk_idx    = [1,   5,   3]    ← which expert indices\n",
    "```\n",
    "\n",
    "```python\n",
    "        gate = F.softmax(topk_logits, dim=-1, dtype=x.dtype)\n",
    "#       └──────────────────────────────────────────────────┘\n",
    "#       Convert scores to probabilities (sum to 1)\n",
    "#       \n",
    "#       Example: [0.9, 0.8, 0.7] → softmax → [0.38, 0.33, 0.29]\n",
    "#                                              └────────────────┘\n",
    "#                                                   sums to 1.0\n",
    "```\n",
    "\n",
    "### Step 5: Expert Dispatch\n",
    "\n",
    "```python\n",
    "        routed_out = torch.zeros_like(x_flat)  # [N, D]\n",
    "#       └────────────────────────────────────┘\n",
    "#       Initialize output buffer with zeros\n",
    "```\n",
    "\n",
    "```python\n",
    "        for i in range(self.n_routed):  # Loop through 16 experts\n",
    "```\n",
    "\n",
    "```python\n",
    "            mask = (topk_idx == i)\n",
    "#           └────────────────────┘\n",
    "#           Boolean mask: where was expert i selected?\n",
    "#           Shape: [N, k] = [128, 8]\n",
    "```\n",
    "\n",
    "```\n",
    "Example (expert i=3):\n",
    "topk_idx = [[1, 5, 3, 7, ...],    ← Token 0 selected experts\n",
    "            [3, 2, 8, 1, ...],    ← Token 1 selected experts\n",
    "            [0, 3, 5, 9, ...],    ← Token 2 selected experts\n",
    "            ...]\n",
    "\n",
    "mask = [[False, False, True, False, ...],   ← expert 3 at position 2\n",
    "        [True, False, False, False, ...],   ← expert 3 at position 0\n",
    "        [False, True, False, False, ...],   ← expert 3 at position 1\n",
    "        ...]\n",
    "```\n",
    "\n",
    "```python\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)\n",
    "#           └─────────────────────────────────────────────┘\n",
    "#           Find indices where mask is True\n",
    "#           \n",
    "#           row_idx: which tokens selected expert i\n",
    "#           which_k: at which position in top-k\n",
    "```\n",
    "\n",
    "```python\n",
    "            if row_idx.numel() == 0:\n",
    "                continue\n",
    "#           └─────────────────────────┘\n",
    "#           Skip if no tokens selected this expert\n",
    "#           .numel() = number of elements\n",
    "```\n",
    "\n",
    "```python\n",
    "            exp_in = x_flat.index_select(0, row_idx)  # [Ti, D]\n",
    "#           └──────────────────────────────────────┘\n",
    "#           Select only the tokens that chose this expert\n",
    "#           \n",
    "#           index_select(dim, indices):\n",
    "#             - dim=0 means select along first dimension (rows)\n",
    "#             - row_idx are the row indices to select\n",
    "```\n",
    "\n",
    "```python\n",
    "            out = self.routed[i](exp_in)  # [Ti, D]\n",
    "#           └──────────────────────────┘\n",
    "#           Pass selected tokens through expert i's FFN\n",
    "```\n",
    "\n",
    "```python\n",
    "            w = gate[row_idx, which_k].unsqueeze(-1)  # [Ti, 1]\n",
    "#           └──────────────────────────────────────┘\n",
    "#           Get the gating weights for these tokens\n",
    "#           \n",
    "#           gate[row_idx, which_k]: select specific elements\n",
    "#           .unsqueeze(-1): add dimension at end [Ti] → [Ti, 1]\n",
    "#                          (needed for broadcasting)\n",
    "```\n",
    "\n",
    "```python\n",
    "            routed_out.index_add_(0, row_idx, out * w)\n",
    "#           └────────────────────────────────────────┘\n",
    "#           Add weighted output back to original positions\n",
    "#           \n",
    "#           index_add_(dim, indices, source):\n",
    "#             - In-place addition at specific indices\n",
    "#             - routed_out[row_idx] += out * w\n",
    "```\n",
    "\n",
    "```\n",
    "Visual of dispatch:\n",
    "                                    Expert 0\n",
    "Token 0 ─────────────────────────▶ ┌───────┐\n",
    "Token 5 ─────────────────────────▶ │ FFN 0 │──▶ weighted output → routed_out[0,5,...]\n",
    "Token 12 ────────────────────────▶ └───────┘\n",
    "\n",
    "                                    Expert 1\n",
    "Token 1 ─────────────────────────▶ ┌───────┐\n",
    "Token 3 ─────────────────────────▶ │ FFN 1 │──▶ weighted output → routed_out[1,3,...]\n",
    "Token 7 ─────────────────────────▶ └───────┘\n",
    "\n",
    "... (repeat for all 16 experts)\n",
    "```\n",
    "\n",
    "### Step 6: Combine Outputs\n",
    "\n",
    "```python\n",
    "        routed_out = routed_out.view(B, S, D)\n",
    "#       └───────────────────────────────────┘\n",
    "#       Reshape back: [128, 1024] → [2, 64, 1024]\n",
    "```\n",
    "\n",
    "```python\n",
    "        return x + shared_out + routed_out\n",
    "#       └─────────────────────────────────┘\n",
    "#       Residual connection: add everything together\n",
    "#       \n",
    "#       output = original_input + shared_experts + routed_experts\n",
    "```\n",
    "\n",
    "```\n",
    "Final combination:\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│                                                 │\n",
    "│   x (input) ──────────────────────────┐         │\n",
    "│        │                              │         │\n",
    "│        ├──▶ Shared Experts ──────────┐│         │\n",
    "│        │                             ││         │\n",
    "│        └──▶ Router + Routed Experts ─┴┴──▶ ADD ─┼──▶ output\n",
    "│                                                 │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Update Bias Method\n",
    "\n",
    "```python\n",
    "    @torch.no_grad()\n",
    "#   └──────────────┘\n",
    "#   Decorator: disable gradient computation\n",
    "#   (we don't want gradients here, just direct updates)\n",
    "    \n",
    "    def update_bias(self, x: torch.Tensor):\n",
    "```\n",
    "\n",
    "```python\n",
    "        N = x.shape[0] * x.shape[1]\n",
    "#       └─────────────────────────────┘\n",
    "#       Total number of tokens = Batch × Sequence\n",
    "```\n",
    "\n",
    "```python\n",
    "        logits = F.linear(x.reshape(-1, self.d_model), self.centroids) + self.bias\n",
    "#       └──────────────────────────────────────────────────────────────────────────┘\n",
    "#       Recalculate router logits (same as in forward)\n",
    "```\n",
    "\n",
    "```python\n",
    "        _, idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "#       └─────────────────────────────────────────────┘\n",
    "#       Get which experts were selected (we don't need the values)\n",
    "#       _ means \"discard this value\"\n",
    "```\n",
    "\n",
    "```python\n",
    "        counts = torch.bincount(idx.flatten(), minlength=self.n_routed).float()\n",
    "#       └─────────────────────────────────────────────────────────────────────┘\n",
    "#       Count how many times each expert was selected\n",
    "#       \n",
    "#       idx.flatten(): convert [128, 8] → [1024] (all selections)\n",
    "#       bincount: count occurrences of each value 0-15\n",
    "#       minlength: ensure output has 16 elements\n",
    "#       .float(): convert to float for math operations\n",
    "```\n",
    "\n",
    "```\n",
    "Example:\n",
    "idx.flatten() = [1, 5, 3, 7, 3, 2, 8, 1, ...]  (1024 values)\n",
    "\n",
    "bincount counts occurrences:\n",
    "Expert 0: appeared 50 times\n",
    "Expert 1: appeared 80 times\n",
    "Expert 2: appeared 45 times\n",
    "...\n",
    "Expert 15: appeared 70 times\n",
    "\n",
    "counts = [50, 80, 45, ..., 70]\n",
    "```\n",
    "\n",
    "```python\n",
    "        avg = counts.sum() / max(1, self.n_routed)\n",
    "#       └────────────────────────────────────────┘\n",
    "#       Calculate average load per expert\n",
    "#       \n",
    "#       Example: sum=1024 (total selections), n_routed=16\n",
    "#                avg = 1024 / 16 = 64 (ideal: each expert gets 64)\n",
    "```\n",
    "\n",
    "```python\n",
    "        violation = (avg - counts) / (avg + 1e-6)\n",
    "#       └───────────────────────────────────────┘\n",
    "#       How far is each expert from the average?\n",
    "#       \n",
    "#       If count < avg: violation > 0 (under-loaded, needs boost)\n",
    "#       If count > avg: violation < 0 (over-loaded, needs penalty)\n",
    "#       \n",
    "#       1e-6 prevents division by zero\n",
    "```\n",
    "\n",
    "```\n",
    "Example:\n",
    "avg = 64\n",
    "counts = [50, 80, 45, 70, ...]\n",
    "\n",
    "violation[0] = (64 - 50) / 64 = +0.22  (under-loaded, increase bias)\n",
    "violation[1] = (64 - 80) / 64 = -0.25  (over-loaded, decrease bias)\n",
    "```\n",
    "\n",
    "```python\n",
    "        self.bias.add_(self.bias_lr * torch.tanh(violation))\n",
    "#       └──────────────────────────────────────────────────┘\n",
    "#       Update bias in-place\n",
    "#       \n",
    "#       torch.tanh(): squash to [-1, 1] range (prevents extreme updates)\n",
    "#       self.bias_lr: step size (0.01 = small steps)\n",
    "#       .add_(): in-place addition (modifies self.bias directly)\n",
    "```\n",
    "\n",
    "```\n",
    "tanh function:\n",
    "                1 ┤         ╭──────────\n",
    "                  │       ╱\n",
    "                0 ┼─────╱─────────────\n",
    "                  │   ╱\n",
    "               -1 ┤──╯\n",
    "                  └─────────────────────\n",
    "                  -3   -1   0    1    3\n",
    "\n",
    "Squashes any value to range [-1, 1]\n",
    "Prevents explosive bias updates!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Testing the Model\n",
    "\n",
    "```python\n",
    "# Configuration\n",
    "d_model = 1024         # Each token is a 1024-dim vector\n",
    "n_routed_exp = 16      # 16 routed experts\n",
    "n_shared_exp = 2       # 2 shared experts\n",
    "top_k = 8              # Select 8 experts per token\n",
    "\n",
    "# Create model\n",
    "model = DeepSeekMoE(d_model, n_routed_exp, n_shared_exp, top_k)\n",
    "\n",
    "# Create random input\n",
    "batch_size_new = 2\n",
    "seq_len_new = 64\n",
    "random_input_new = torch.randn(batch_size_new, seq_len_new, d_model)\n",
    "#                  └───────────────────────────────────────────────┘\n",
    "#                  Random tensor from normal distribution\n",
    "#                  Shape: [2, 64, 1024]\n",
    "\n",
    "# Forward pass\n",
    "output_new = model(random_input_new)\n",
    "\n",
    "print(\"New output shape:\", output_new.shape)\n",
    "# Output: torch.Size([2, 64, 1024])  ← Same shape as input!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference: Key PyTorch Operations\n",
    "\n",
    "| Operation | What It Does | Example |\n",
    "|-----------|--------------|---------|\n",
    "| `tensor.shape` | Get dimensions | `[2, 64, 1024]` |\n",
    "| `tensor.reshape(-1, D)` | Flatten to 2D | `[128, 1024]` |\n",
    "| `tensor.view(B, S, D)` | Reshape (must be contiguous) | `[2, 64, 1024]` |\n",
    "| `F.linear(x, w)` | Matrix multiply: `x @ w.T` | `[N, D] @ [E, D].T → [N, E]` |\n",
    "| `F.softmax(x, dim=-1)` | Normalize to probabilities | Sum to 1.0 |\n",
    "| `torch.topk(x, k)` | Get k largest values | Values and indices |\n",
    "| `tensor.index_select(0, idx)` | Select rows by index | Subset of tensor |\n",
    "| `tensor.index_add_(0, idx, src)` | Add to specific rows | In-place accumulate |\n",
    "| `torch.zeros_like(x)` | Zero tensor same shape as x | Initialization |\n",
    "| `torch.bincount(x)` | Count occurrences | Histogram |\n",
    "| `torch.tanh(x)` | Hyperbolic tangent | Squash to [-1, 1] |\n",
    "| `tensor.add_(x)` | In-place addition | Modify in place |\n",
    "| `@torch.no_grad()` | Disable gradients | For inference/manual updates |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The code implements a **Mixture of Experts** layer where:\n",
    "\n",
    "1. **Shared experts** always process all tokens\n",
    "2. **Router** calculates which routed experts each token should use\n",
    "3. **Top-k selection** picks the best experts for each token\n",
    "4. **Gating** weights the contribution of each selected expert\n",
    "5. **Dispatch** sends tokens to their selected experts\n",
    "6. **Combine** adds everything together with a residual connection\n",
    "7. **Bias update** keeps expert loads balanced over time\n",
    "\n",
    "This achieves **large model capacity** with **efficient computation!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
