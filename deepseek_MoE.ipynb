{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adf2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math \n",
    "from contextlib import nullcontext \n",
    "from typing import Optional \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0429ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Slightly faster GELU (approx)\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) *\n",
    "                                       (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class ExpertFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer MLP expert. Hidden dim is usually smaller than a dense FFN\n",
    "    (e.g., 0.25 Ã— d_model in DeepSeek-V3).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, hidden: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.dropout(_gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSeek-V3 style Mixture-of-Experts (MoE) layer.\n",
    "\n",
    "    This MoE layer incorporates both routed experts (selected by a router)\n",
    "    and shared experts (applied to all inputs). It is designed based on\n",
    "    the architecture described in the DeepSeek-V3 paper.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output features.\n",
    "        n_routed_exp (int): The number of routed experts.\n",
    "        n_shared_exp (int, optional): The number of shared experts. Defaults to 1.\n",
    "        top_k (int, optional): The number of routed experts to select for each token.\n",
    "                               Defaults to 8.\n",
    "        routed_hidden (int, optional): The hidden dimension for routed experts.\n",
    "                                      Defaults to 2048.\n",
    "        shared_hidden (Optional[int], optional): The hidden dimension for shared experts.\n",
    "                                                If None, uses routed_hidden. Defaults to None.\n",
    "        bias_lr (float, optional): Learning rate for the router bias (updated online).\n",
    "                                   Defaults to 0.01.\n",
    "        fp16_router (bool, optional): Whether to use FP16 precision for router calculations.\n",
    "                                     Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_routed_exp: int,\n",
    "        n_shared_exp: int = 1,\n",
    "        top_k: int = 8,\n",
    "        routed_hidden: int = 2_048,\n",
    "        shared_hidden: Optional[int] = None,\n",
    "        bias_lr: float = 0.01,\n",
    "        fp16_router: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Assert that the number of selected experts (top_k) is less than or equal to the total number of routed experts.\n",
    "        assert top_k <= n_routed_exp, \"k must be â‰¤ number of routed experts\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_routed = n_routed_exp\n",
    "        self.n_shared = n_shared_exp\n",
    "        self.top_k = top_k\n",
    "        self.bias_lr = bias_lr\n",
    "        self.fp16_router = fp16_router\n",
    "\n",
    "        # Module list for the routed experts.\n",
    "        self.routed = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "        )\n",
    "        # Determine the hidden dimension for shared experts. Use routed_hidden if shared_hidden is not provided.\n",
    "        hidden_shared = shared_hidden or routed_hidden\n",
    "        # Module list for the shared experts.\n",
    "        self.shared = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, hidden_shared) for _ in range(n_shared_exp)]\n",
    "        )\n",
    "\n",
    "        # Register a parameter for the centroids used by the router.\n",
    "        # Centroids represent the \"preference\" of each expert for different input features.\n",
    "        self.register_parameter(\"centroids\", nn.Parameter(torch.empty(n_routed_exp, d_model)))\n",
    "        # Initialize centroids with a normal distribution.\n",
    "        nn.init.normal_(self.centroids, std=d_model ** -0.5)\n",
    "\n",
    "        # Register a buffer for the router bias. This bias is updated online\n",
    "        # without using standard gradient descent, hence it's not a parameter.\n",
    "        self.register_buffer(\"bias\", torch.zeros(n_routed_exp))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DeepSeekMoE layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], where B is\n",
    "                              batch size, S is sequence length, and D is d_model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape [B, S, D], which is the\n",
    "                          sum of the input, shared expert outputs, and routed\n",
    "                          expert outputs.\n",
    "        \"\"\"\n",
    "        # Get dimensions of the input tensor.\n",
    "        B, S, D = x.shape\n",
    "        # Reshape the input to [N, D], where N = B * S (number of tokens).\n",
    "        x_flat = x.reshape(-1, D)  # [N, D] with N=B*S\n",
    "\n",
    "        # 1) Shared path: Process the input through all shared experts and sum their outputs.\n",
    "        shared_out = torch.zeros_like(x)\n",
    "        for exp in self.shared:\n",
    "            shared_out += exp(x)\n",
    "        # (Optional) Scale the shared expert output by the number of shared experts.\n",
    "        # This can help in balancing the contribution of shared vs. routed experts.\n",
    "        # shared_out = shared_out / max(1, self.n_shared)\n",
    "\n",
    "        # 2) Router logits: Calculate the affinity of each token to each routed expert.\n",
    "        # Use autocasting to FP16 if fp16_router is True and the device is CUDA.\n",
    "        use_autocast = self.fp16_router and x.is_cuda\n",
    "        device_type = \"cuda\" if x.is_cuda else x.device.type\n",
    "        with torch.autocast(device_type=device_type, enabled=use_autocast):\n",
    "            # Calculate logits by taking the dot product of the flattened input with the expert centroids.\n",
    "            logits = F.linear(x_flat, self.centroids)  # [N, E]\n",
    "            # Add the router bias to the logits. Ensure bias matches the logits' dtype.\n",
    "            logits = logits + self.bias.to(logits.dtype)\n",
    "\n",
    "        # Select the top_k experts with the highest logits for each token.\n",
    "        topk_logits, topk_idx = torch.topk(logits, self.top_k, dim=-1)        # [N, k]\n",
    "        # Apply softmax to the top_k logits to get gating weights.\n",
    "        # Ensure the gate weights have the same dtype as the input for subsequent calculations.\n",
    "        gate = F.softmax(topk_logits, dim=-1, dtype=x.dtype)                   # [N, k]\n",
    "\n",
    "        # 3) Dispatch per expert: Route tokens to their selected experts and combine outputs.\n",
    "        routed_out = torch.zeros_like(x_flat)                                   # [N, D]\n",
    "        # Iterate through each routed expert.\n",
    "        for i in range(self.n_routed):\n",
    "            # Create a mask to identify which tokens selected the current expert (expert i).\n",
    "            mask = (topk_idx == i)\n",
    "            # Find the indices of the rows (tokens) and columns (which of the top-k) where expert i was selected.\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)                      # 1-D each\n",
    "            # If no tokens selected this expert, skip.\n",
    "            if row_idx.numel() == 0:\n",
    "                continue\n",
    "            # Select the input tokens that are routed to expert i.\n",
    "            exp_in = x_flat.index_select(0, row_idx)                            # [Ti, D] where Ti is the number of tokens routed to expert i\n",
    "            # Pass the selected tokens through the expert's FFN.\n",
    "            out = self.routed[i](exp_in)                                        # [Ti, D]\n",
    "            # Get the gating weights for the tokens routed to expert i.\n",
    "            w = gate[row_idx, which_k].unsqueeze(-1)                            # [Ti, 1]\n",
    "            # Scale the expert output by the gating weights and add it to the routed_out tensor\n",
    "            # at the original token positions using index_add_.\n",
    "            routed_out.index_add_(0, row_idx, out * w)\n",
    "\n",
    "        # Reshape the routed output back to the original [B, S, D] shape.\n",
    "        routed_out = routed_out.view(B, S, D)\n",
    "        # The final output is the sum of the original input, shared expert outputs, and routed expert outputs.\n",
    "        return x + shared_out + routed_out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_bias(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Updates the router bias based on expert load.\n",
    "\n",
    "        This method is typically called once per optimizer step using the\n",
    "        same batch of tokens that were passed through the forward method.\n",
    "        It uses the current router logits (including the current bias) to\n",
    "        estimate the load on each expert and adjusts the bias to encourage\n",
    "        a more balanced distribution of tokens across experts.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], identical\n",
    "                              to the input used in the corresponding forward pass.\n",
    "        \"\"\"\n",
    "        # Calculate the total number of tokens.\n",
    "        N = x.shape[0] * x.shape[1]\n",
    "        # Calculate the router logits (affinity scores) for each token and expert, including the current bias.\n",
    "        logits = F.linear(x.reshape(-1, self.d_model), self.centroids) + self.bias\n",
    "        # Determine the top_k experts selected for each token based on the current logits.\n",
    "        _, idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "\n",
    "        # Count how many times each expert was selected as one of the top_k.\n",
    "        counts = torch.bincount(idx.flatten(), minlength=self.n_routed).float()\n",
    "        # Calculate the average number of times an expert should ideally be selected.\n",
    "        avg = counts.sum() / max(1, self.n_routed)\n",
    "\n",
    "        # Calculate the \"violation\" for each expert. A positive violation means\n",
    "        # the expert is under-loaded compared to the average, and its bias\n",
    "        # should be increased to make it more likely to be selected in the future.\n",
    "        # A negative violation means it's over-loaded, and its bias should be decreased.\n",
    "        # Add a small epsilon (1e-6) to the denominator to avoid division by zero.\n",
    "        violation = (avg - counts) / (avg + 1e-6)\n",
    "        # Update the bias using a smooth, bounded update based on the violation.\n",
    "        # torch.tanh() squashes the violation into the range [-1, 1], preventing\n",
    "        # excessively large bias updates. The bias_lr controls the step size.\n",
    "        self.bias.add_(self.bias_lr * torch.tanh(violation))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c0a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New output shape: torch.Size([2, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the DeepSeekMoE class\n",
    "d_model = 1024\n",
    "n_routed_exp = 16\n",
    "n_shared_exp = 2\n",
    "top_k = 8\n",
    "\n",
    "model = DeepSeekMoE(d_model, n_routed_exp, n_shared_exp, top_k)\n",
    "\n",
    "# Create different random input data\n",
    "batch_size_new = 2\n",
    "seq_len_new = 64\n",
    "random_input_new = torch.randn(batch_size_new, seq_len_new, d_model)\n",
    "\n",
    "# Pass the new random input to the model's forward method\n",
    "output_new = model(random_input_new)\n",
    "\n",
    "print(\"New output shape:\", output_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e93705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5df50099",
   "metadata": {},
   "source": [
    "# ğŸ§  DeepSeek Mixture of Experts (MoE) - Simple Explanation\n",
    "\n",
    "## ğŸ¯ The Main Goal\n",
    "\n",
    "**Problem:** Large neural networks are expensive to run because every input goes through ALL parameters.\n",
    "\n",
    "**Solution:** Use **Mixture of Experts (MoE)** - have many small \"expert\" networks, but only use a FEW of them for each input!\n",
    "\n",
    "```\n",
    "Traditional Dense Network:          MoE Network:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Every input uses   â”‚            â”‚  Each input only    â”‚\n",
    "â”‚  ALL parameters     â”‚            â”‚  uses SELECTED      â”‚\n",
    "â”‚  (expensive! ğŸ’¸)    â”‚            â”‚  experts (cheap! âœ¨) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Architecture Overview\n",
    "\n",
    "```\n",
    "                         Input Token\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚                               â”‚\n",
    "              â–¼                               â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚   SHARED     â”‚               â”‚   ROUTER     â”‚\n",
    "      â”‚   EXPERTS    â”‚               â”‚  (Gatekeeper)â”‚\n",
    "      â”‚ (Always ON)  â”‚               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "             â”‚                    Picks Top-K Experts\n",
    "             â”‚                              â”‚\n",
    "             â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”‚                    â–¼         â–¼         â–¼\n",
    "             â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”‚               â”‚Expert 1â”‚ â”‚Expert 5â”‚ â”‚Expert 9â”‚  â† Only these\n",
    "             â”‚               â”‚  ğŸ”µ    â”‚ â”‚  ğŸŸ¢    â”‚ â”‚  ğŸŸ£    â”‚    are activated!\n",
    "             â”‚               â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜\n",
    "             â”‚                    â”‚         â”‚         â”‚\n",
    "             â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚                              â”‚\n",
    "             â”‚                    Weighted Sum (gating)\n",
    "             â”‚                              â”‚\n",
    "             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚\n",
    "                            â–¼\n",
    "                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                      â”‚   ADD     â”‚  â† Combine all outputs\n",
    "                      â”‚   ALL     â”‚\n",
    "                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚\n",
    "                            â–¼\n",
    "                     Final Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Key Components Explained\n",
    "\n",
    "### 1ï¸âƒ£ Expert FFN (Feed-Forward Network)\n",
    "\n",
    "Each expert is a **small 2-layer neural network**:\n",
    "\n",
    "```\n",
    "Input â”€â”€â–¶ [Linear Layer 1] â”€â”€â–¶ [GELU Activation] â”€â”€â–¶ [Linear Layer 2] â”€â”€â–¶ Output\n",
    "         (expand)              (non-linearity)       (compress back)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Simple: Input â†’ Hidden â†’ Output\n",
    "fc1: d_model â†’ hidden    # Expand dimensions\n",
    "GELU: activation         # Add non-linearity  \n",
    "fc2: hidden â†’ d_model    # Back to original size\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Shared Experts (Always Active)\n",
    "\n",
    "These experts process **EVERY token** - they're always \"on duty\":\n",
    "\n",
    "```\n",
    "Token 1 â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "Token 2 â”€â”€â–¶ â”‚  Shared Expert  â”‚ â”€â”€â–¶ ALL tokens get processed\n",
    "Token 3 â”€â”€â–¶ â”‚    (Always ON)  â”‚\n",
    "   ...  â”€â”€â–¶ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Why?** They capture general patterns that apply to all inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Routed Experts (Selectively Active)\n",
    "\n",
    "The **Router** decides which experts to use for each token:\n",
    "\n",
    "```\n",
    "        16 Routed Experts Available\n",
    "    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "    â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚ ...\n",
    "    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "          â”‚       â”‚   â”‚           â”‚\n",
    "          â–¼       â–¼   â–¼           â–¼\n",
    "        Token A picks: Expert 2, 4, 5, 8 (top_k = 4)\n",
    "        \n",
    "    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "    â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚ ...\n",
    "    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "      â”‚           â”‚       â”‚   â”‚\n",
    "      â–¼           â–¼       â–¼   â–¼\n",
    "    Token B picks: Expert 1, 4, 6, 7 (different experts!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ The Router (Gatekeeper)\n",
    "\n",
    "The router calculates **affinity scores** between each token and each expert:\n",
    "\n",
    "```\n",
    "                    How Router Works\n",
    "                    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                    \n",
    "Token Embedding â”€â”€â–¶ Dot Product with Expert Centroids â”€â”€â–¶ Scores\n",
    "     [1024]              [16 Ã— 1024]                      [16]\n",
    "                    \n",
    "     \"How similar is this token to each expert's specialty?\"\n",
    "```\n",
    "\n",
    "```\n",
    "Score Calculation:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  logits = token Â· centroids + bias         â”‚\n",
    "â”‚                                            â”‚\n",
    "â”‚  Token: \"The cat sat...\"                   â”‚\n",
    "â”‚                                            â”‚\n",
    "â”‚  Expert 1 (grammar):     score = 0.8  â­   â”‚\n",
    "â”‚  Expert 2 (math):        score = 0.1       â”‚\n",
    "â”‚  Expert 3 (code):        score = 0.2       â”‚\n",
    "â”‚  Expert 4 (language):    score = 0.9  â­   â”‚\n",
    "â”‚  ...                                       â”‚\n",
    "â”‚                                            â”‚\n",
    "â”‚  â†’ Select TOP-K highest scores!            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ Gating Weights (Softmax)\n",
    "\n",
    "After selecting top-k experts, we **weight their contributions**:\n",
    "\n",
    "```\n",
    "Selected Experts:    Expert 4    Expert 1    Expert 7    Expert 2\n",
    "Top-k Logits:          0.9         0.8         0.6         0.5\n",
    "                        â”‚           â”‚           â”‚           â”‚\n",
    "                        â–¼           â–¼           â–¼           â–¼\n",
    "                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Softmax â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        â”‚           â”‚           â”‚           â”‚\n",
    "                        â–¼           â–¼           â–¼           â–¼\n",
    "Gate Weights:         0.35        0.30        0.20        0.15\n",
    "                        â”‚           â”‚           â”‚           â”‚\n",
    "                        â”‚     (Higher score = More influence)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Forward Pass Flow\n",
    "\n",
    "```\n",
    "Step 1: Input arrives\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  x: [Batch, Seq, 1024]  â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "Step 2: Shared experts process ALL tokens\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  shared_out = Î£ exp(x)  â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "Step 3: Router calculates expert preferences\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  logits = x Â· centroids â”‚\n",
    "        â”‚  + bias                 â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "Step 4: Select top-k experts per token\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  top_k = 8 experts      â”‚\n",
    "        â”‚  (out of 16 available)  â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "Step 5: Route tokens to selected experts\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Each expert processes  â”‚\n",
    "        â”‚  only ITS tokens        â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "Step 6: Weight and combine outputs\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  routed_out = Î£(w * out)â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "Step 7: Final output = input + shared + routed\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  return x + shared_out  â”‚\n",
    "        â”‚         + routed_out    â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ Load Balancing (Bias Update)\n",
    "\n",
    "**Problem:** Some experts might get ALL the tokens (overloaded), while others get NONE (underused).\n",
    "\n",
    "**Solution:** Dynamically adjust the router bias!\n",
    "\n",
    "```\n",
    "                Expert Load Distribution\n",
    "    \n",
    "    Before Balancing:                After Balancing:\n",
    "    \n",
    "    Expert 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (120)     Expert 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (80)\n",
    "    Expert 2: â–ˆâ–ˆ (20)                Expert 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (70)\n",
    "    Expert 3: â–ˆ (10)                 Expert 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (75)\n",
    "    Expert 4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (160) Expert 4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (75)\n",
    "    \n",
    "    Unbalanced! ğŸ˜Ÿ                    Balanced! ğŸ˜Š\n",
    "```\n",
    "\n",
    "```\n",
    "How Bias Update Works:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. Count how many tokens each expert received\n",
    "2. Calculate average load\n",
    "3. If expert is UNDER-loaded â†’ INCREASE its bias (make it more attractive)\n",
    "4. If expert is OVER-loaded â†’ DECREASE its bias (make it less attractive)\n",
    "\n",
    "Formula: bias += lr * tanh((avg - count) / avg)\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 Smooth, bounded update\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Example with Numbers\n",
    "\n",
    "```python\n",
    "# Configuration\n",
    "d_model = 1024        # Token dimension\n",
    "n_routed_exp = 16     # 16 routed experts available  \n",
    "n_shared_exp = 2      # 2 shared experts (always active)\n",
    "top_k = 8             # Select 8 experts per token\n",
    "\n",
    "# Input\n",
    "batch_size = 2\n",
    "seq_len = 64\n",
    "# Total tokens = 2 Ã— 64 = 128 tokens\n",
    "```\n",
    "\n",
    "```\n",
    "For each of 128 tokens:\n",
    "â”œâ”€â”€ 2 shared experts process it (ALWAYS)\n",
    "â””â”€â”€ Router selects 8 out of 16 routed experts\n",
    "    â””â”€â”€ These 8 experts process it with weighted contributions\n",
    "\n",
    "Computation Savings:\n",
    "â”œâ”€â”€ Dense model: ALL parameters for ALL tokens\n",
    "â””â”€â”€ MoE model: Only 8/16 = 50% of routed experts per token! ğŸ‰\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¨ Visual Summary\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DeepSeek MoE Layer                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   INPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\n",
    "â”‚                   â”‚                     â”‚                       â”‚\n",
    "â”‚                   â–¼                     â–¼                       â”‚\n",
    "â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚            â”‚   SHARED     â”‚      â”‚   ROUTER    â”‚                â”‚\n",
    "â”‚            â”‚   EXPERTS    â”‚      â”‚  + GATING   â”‚                â”‚\n",
    "â”‚            â”‚  (2 always)  â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚                       â”‚\n",
    "â”‚                   â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚                   â”‚              â”‚ TOP-K = 8   â”‚                â”‚\n",
    "â”‚                   â”‚              â”‚ Selection   â”‚                â”‚\n",
    "â”‚                   â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                   â”‚                     â”‚                       â”‚\n",
    "â”‚                   â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚                   â”‚              â”‚  8 ROUTED   â”‚                â”‚\n",
    "â”‚                   â”‚              â”‚  EXPERTS    â”‚                â”‚\n",
    "â”‚                   â”‚              â”‚  (selected) â”‚                â”‚\n",
    "â”‚                   â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                   â”‚                     â”‚                       â”‚\n",
    "â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\n",
    "â”‚                             â”‚                                   â”‚\n",
    "â”‚                        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                              â”‚\n",
    "â”‚                        â”‚   ADD   â”‚                              â”‚\n",
    "â”‚                        â”‚ (x + s  â”‚                              â”‚\n",
    "â”‚                        â”‚  + r)   â”‚                              â”‚\n",
    "â”‚                        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                              â”‚\n",
    "â”‚                             â”‚                                   â”‚\n",
    "â”‚                          OUTPUT                                 â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Key Takeaways\n",
    "\n",
    "| Concept | What It Does |\n",
    "|---------|--------------|\n",
    "| **Shared Experts** | Always active, capture common patterns |\n",
    "| **Routed Experts** | Specialized, only activated when needed |\n",
    "| **Router** | Decides which experts to use per token |\n",
    "| **Top-K Selection** | Limits computation by selecting few experts |\n",
    "| **Gating** | Weights expert outputs by relevance |\n",
    "| **Bias Update** | Keeps expert loads balanced |\n",
    "| **Residual Connection** | Output = Input + Shared + Routed |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Why This Matters\n",
    "\n",
    "```\n",
    "Traditional Transformer FFN:     vs     MoE FFN:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "All 256 experts active                 Only 8 experts active\n",
    "= 256Ã— computation                     = 8Ã— computation\n",
    "= SLOW + EXPENSIVE                     = FAST + CHEAP! âœ¨\n",
    "\n",
    "Same model capacity, much less compute!\n",
    "```\n",
    "\n",
    "This is how DeepSeek-V3 achieves **large model capacity** with **efficient inference**! ğŸ¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56974d16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1e7d496",
   "metadata": {},
   "source": [
    "# ğŸ” DeepSeek MoE - Code Walkthrough\n",
    "\n",
    "This document explains **every line of code** in simple terms.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ 1. Imports\n",
    "\n",
    "```python\n",
    "import math \n",
    "from contextlib import nullcontext \n",
    "from typing import Optional \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "```\n",
    "\n",
    "| Import | What It Does |\n",
    "|--------|--------------|\n",
    "| `math` | Python's math library (for `sqrt`, `pi`, etc.) |\n",
    "| `nullcontext` | A \"do nothing\" context manager (placeholder) |\n",
    "| `Optional` | Type hint meaning \"this can be None\" |\n",
    "| `torch` | PyTorch - the deep learning framework |\n",
    "| `torch.nn` | Neural network building blocks (layers, modules) |\n",
    "| `torch.nn.functional as F` | Functional operations (softmax, linear, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® 2. GELU Activation Function\n",
    "\n",
    "```python\n",
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) *\n",
    "                                       (x + 0.044715 * torch.pow(x, 3))))\n",
    "```\n",
    "\n",
    "### What is GELU?\n",
    "GELU = **G**aussian **E**rror **L**inear **U**nit\n",
    "\n",
    "It's a smooth activation function (like ReLU but smoother):\n",
    "\n",
    "```\n",
    "Input x:     -2    -1     0     1     2\n",
    "GELU(x):   -0.04  -0.16   0    0.84  1.95\n",
    "\n",
    "         ReLU:              GELU:\n",
    "         â”‚    /             â”‚    â•±\n",
    "         â”‚   /              â”‚  â•±\n",
    "    â”€â”€â”€â”€â”€â”¼â”€â”€/â”€â”€â”€â”€      â”€â”€â”€â”€â•±â”¼â”€â”€â”€â”€â”€\n",
    "         â”‚                â•±  â”‚\n",
    "         â”‚              â•±    â”‚\n",
    "```\n",
    "\n",
    "### Code Breakdown:\n",
    "\n",
    "```python\n",
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "#   â””â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#   function name         returns a tensor\n",
    "#          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#          x is a tensor (type hint)\n",
    "```\n",
    "\n",
    "```python\n",
    "return 0.5 * x * (1.0 + torch.tanh(...))\n",
    "#      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#      This is the approximate GELU formula\n",
    "#      (faster than exact GELU)\n",
    "```\n",
    "\n",
    "```python\n",
    "math.sqrt(2.0 / math.pi)  # â‰ˆ 0.7979 (a constant)\n",
    "x + 0.044715 * torch.pow(x, 3)  # x + 0.044715 * xÂ³\n",
    "#                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#                 x raised to power 3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  3. ExpertFFN Class (Feed-Forward Network)\n",
    "\n",
    "```python\n",
    "class ExpertFFN(nn.Module):\n",
    "```\n",
    "- `class ExpertFFN` â†’ Define a new class called ExpertFFN\n",
    "- `nn.Module` â†’ Inherit from PyTorch's base neural network class\n",
    "\n",
    "### Constructor (`__init__`)\n",
    "\n",
    "```python\n",
    "def __init__(self, d_model: int, hidden: int, dropout: float = 0.0):\n",
    "#            â””â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#            self   input dim    hidden dim   dropout rate (default 0)\n",
    "    \n",
    "    super().__init__()\n",
    "#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#   Call parent class constructor (required for nn.Module)\n",
    "```\n",
    "\n",
    "```python\n",
    "    self.fc1 = nn.Linear(d_model, hidden, bias=False)\n",
    "#   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#   save as    Create a linear layer: d_model â†’ hidden\n",
    "#   attribute  bias=False means no bias term (just weights)\n",
    "```\n",
    "\n",
    "```\n",
    "Linear Layer Visualization:\n",
    "                    \n",
    "  Input [d_model]          Weights [d_model Ã— hidden]         Output [hidden]\n",
    "  â”Œâ”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”\n",
    "  â”‚ xâ‚â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Wâ‚â‚  Wâ‚â‚‚  ...  Wâ‚â‚• â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ yâ‚â”‚\n",
    "  â”‚ xâ‚‚â”‚                    â”‚  Wâ‚‚â‚  Wâ‚‚â‚‚  ...  Wâ‚‚â‚• â”‚           â”‚ yâ‚‚â”‚\n",
    "  â”‚...â”‚                    â”‚  ...  ...  ...  ... â”‚           â”‚...â”‚\n",
    "  â”‚ xâ‚™â”‚                    â”‚  Wâ‚™â‚  Wâ‚™â‚‚  ...  Wâ‚™â‚• â”‚           â”‚ yâ‚•â”‚\n",
    "  â””â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”˜\n",
    "  \n",
    "  Formula: y = x @ W  (matrix multiplication)\n",
    "```\n",
    "\n",
    "```python\n",
    "    self.fc2 = nn.Linear(hidden, d_model, bias=False)\n",
    "#   Second linear layer: hidden â†’ d_model (back to original size)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "#   Dropout layer: randomly zeros some neurons during training\n",
    "#   (helps prevent overfitting)\n",
    "```\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "```python\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.fc2(self.dropout(_gelu(self.fc1(x))))\n",
    "#          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#          Chain of operations (read inside-out):\n",
    "```\n",
    "\n",
    "```\n",
    "Step-by-step:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1. self.fc1(x)           â†’  x goes through first linear     â”‚\n",
    "â”‚  2. _gelu(...)            â†’  apply GELU activation           â”‚\n",
    "â”‚  3. self.dropout(...)     â†’  randomly drop some values       â”‚\n",
    "â”‚  4. self.fc2(...)         â†’  second linear layer (output)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "x [d_model] â†’ fc1 â†’ [hidden] â†’ GELU â†’ dropout â†’ fc2 â†’ [d_model]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›ï¸ 4. DeepSeekMoE Class - Constructor\n",
    "\n",
    "```python\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,           # Dimension of input/output (e.g., 1024)\n",
    "        n_routed_exp: int,      # Number of routed experts (e.g., 16)\n",
    "        n_shared_exp: int = 1,  # Number of shared experts (default 1)\n",
    "        top_k: int = 8,         # How many experts to select per token\n",
    "        routed_hidden: int = 2_048,  # Hidden dim for routed experts\n",
    "        shared_hidden: Optional[int] = None,  # Hidden dim for shared (or None)\n",
    "        bias_lr: float = 0.01,  # Learning rate for bias update\n",
    "        fp16_router: bool = False,  # Use FP16 for router? (faster on GPU)\n",
    "    ):\n",
    "```\n",
    "\n",
    "### Assertions and Attributes\n",
    "\n",
    "```python\n",
    "        super().__init__()\n",
    "        \n",
    "        assert top_k <= n_routed_exp, \"k must be â‰¤ number of routed experts\"\n",
    "#       â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       check  condition to check    error message if condition is False\n",
    "```\n",
    "\n",
    "```python\n",
    "        # Store all parameters as instance attributes\n",
    "        self.d_model = d_model        # Save for later use\n",
    "        self.n_routed = n_routed_exp  # Number of routed experts\n",
    "        self.n_shared = n_shared_exp  # Number of shared experts\n",
    "        self.top_k = top_k            # K in \"top-k\" selection\n",
    "        self.bias_lr = bias_lr        # Bias learning rate\n",
    "        self.fp16_router = fp16_router  # FP16 flag\n",
    "```\n",
    "\n",
    "### Creating Expert Networks\n",
    "\n",
    "```python\n",
    "        self.routed = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "        )\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "```python\n",
    "nn.ModuleList([...])\n",
    "#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#   A list of nn.Modules that PyTorch can track\n",
    "#   (regular Python list won't work for gradient tracking!)\n",
    "```\n",
    "\n",
    "```python\n",
    "[ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "#â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#         List comprehension: create n_routed_exp experts\n",
    "```\n",
    "\n",
    "```\n",
    "Visual:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  self.routed = [Expertâ‚€, Expertâ‚, Expertâ‚‚, ... Expertâ‚â‚…]   â”‚\n",
    "â”‚                    â”‚        â”‚        â”‚           â”‚          â”‚\n",
    "â”‚                  FFN      FFN      FFN         FFN          â”‚\n",
    "â”‚                (1024â†’   (1024â†’   (1024â†’      (1024â†’         â”‚\n",
    "â”‚                 2048â†’    2048â†’    2048â†’       2048â†’         â”‚\n",
    "â”‚                 1024)    1024)    1024)       1024)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "```python\n",
    "        hidden_shared = shared_hidden or routed_hidden\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       If shared_hidden is None, use routed_hidden instead\n",
    "#       (Python's \"or\" returns first truthy value)\n",
    "        \n",
    "        self.shared = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, hidden_shared) for _ in range(n_shared_exp)]\n",
    "        )\n",
    "#       Same pattern: create n_shared_exp shared experts\n",
    "```\n",
    "\n",
    "### Router Components\n",
    "\n",
    "```python\n",
    "        self.register_parameter(\"centroids\", \n",
    "                                nn.Parameter(torch.empty(n_routed_exp, d_model)))\n",
    "```\n",
    "\n",
    "**What is this?**\n",
    "\n",
    "```python\n",
    "register_parameter(\"name\", parameter)\n",
    "#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#   Register a learnable parameter with PyTorch\n",
    "#   (will be updated during training via gradients)\n",
    "```\n",
    "\n",
    "```python\n",
    "torch.empty(n_routed_exp, d_model)\n",
    "#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#   Create uninitialized tensor of shape [16, 1024]\n",
    "#   (will be initialized next)\n",
    "```\n",
    "\n",
    "```\n",
    "Centroids shape: [n_routed_exp, d_model] = [16, 1024]\n",
    "\n",
    "Each row = one expert's \"centroid\" (preference vector)\n",
    "\n",
    "         d_model = 1024 dimensions\n",
    "         â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â†‘\n",
    "    â”‚  Expert 0's centroid vector    â”‚  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "    â”‚  Expert 1's centroid vector    â”‚  â”‚ n_routed_exp\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚ = 16 experts\n",
    "    â”‚           ...                  â”‚  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "    â”‚  Expert 15's centroid vector   â”‚  â†“\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "```python\n",
    "        nn.init.normal_(self.centroids, std=d_model ** -0.5)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Initialize with normal distribution\n",
    "#       std = 1/âˆš1024 â‰ˆ 0.031 (small values)\n",
    "```\n",
    "\n",
    "```python\n",
    "        self.register_buffer(\"bias\", torch.zeros(n_routed_exp))\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       register_buffer = NOT a learnable parameter\n",
    "#       (won't get gradients, but will be saved with model)\n",
    "#       \n",
    "#       torch.zeros(16) = [0, 0, 0, ... 0] (16 zeros)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ 5. Forward Pass - Step by Step\n",
    "\n",
    "```python\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "```\n",
    "\n",
    "### Step 1: Get Dimensions\n",
    "\n",
    "```python\n",
    "        B, S, D = x.shape\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Unpack shape: Batch size, Sequence length, Dimension\n",
    "#       e.g., x.shape = [2, 64, 1024]\n",
    "#             B=2, S=64, D=1024\n",
    "```\n",
    "\n",
    "```python\n",
    "        x_flat = x.reshape(-1, D)  # [N, D] with N=B*S\n",
    "#               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Flatten first two dimensions\n",
    "#       [2, 64, 1024] â†’ [128, 1024]\n",
    "#       \n",
    "#       -1 means \"calculate this dimension automatically\"\n",
    "```\n",
    "\n",
    "```\n",
    "Before:                          After:\n",
    "x: [B, S, D]                     x_flat: [N, D]\n",
    "   [2, 64, 1024]                         [128, 1024]\n",
    "   \n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  Batch 0    â”‚               â”‚  Token 0    â”‚\n",
    "   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚  Token 1    â”‚\n",
    "   â”‚ â”‚Token 0-63â”‚ â”‚   flatten    â”‚  Token 2    â”‚\n",
    "   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”€â”€â”€â”€â”€â”€â”€â–¶    â”‚    ...      â”‚\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚  Token 127  â”‚\n",
    "   â”‚  Batch 1    â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               \n",
    "   â”‚ â”‚Token 0-63â”‚ â”‚               N = 2Ã—64 = 128\n",
    "   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               \n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               \n",
    "```\n",
    "\n",
    "### Step 2: Shared Experts Path\n",
    "\n",
    "```python\n",
    "        shared_out = torch.zeros_like(x)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Create zero tensor with same shape as x: [B, S, D]\n",
    "```\n",
    "\n",
    "```python\n",
    "        for exp in self.shared:\n",
    "            shared_out += exp(x)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Loop through each shared expert\n",
    "#       Add each expert's output to shared_out\n",
    "```\n",
    "\n",
    "```\n",
    "Visual:\n",
    "        x â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â–¶ Shared Expert 0 â”€â”€â–¶ outputâ‚€\n",
    "                â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â–¶ Shared Expert 1 â”€â”€â–¶ outputâ‚\n",
    "                \n",
    "        shared_out = outputâ‚€ + outputâ‚\n",
    "```\n",
    "\n",
    "### Step 3: Router Logits\n",
    "\n",
    "```python\n",
    "        use_autocast = self.fp16_router and x.is_cuda\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Use FP16 only if flag is True AND on GPU\n",
    "        \n",
    "        device_type = \"cuda\" if x.is_cuda else x.device.type\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Get device type string for autocast\n",
    "```\n",
    "\n",
    "```python\n",
    "        with torch.autocast(device_type=device_type, enabled=use_autocast):\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Context manager for mixed precision (FP16)\n",
    "#       Makes computation faster on modern GPUs\n",
    "```\n",
    "\n",
    "```python\n",
    "            logits = F.linear(x_flat, self.centroids)  # [N, E]\n",
    "#                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Compute dot product between tokens and expert centroids\n",
    "#           \n",
    "#           x_flat:    [N, D] = [128, 1024]\n",
    "#           centroids: [E, D] = [16, 1024]\n",
    "#           logits:    [N, E] = [128, 16]\n",
    "```\n",
    "\n",
    "```\n",
    "F.linear(input, weight) = input @ weight.T\n",
    "\n",
    "For each token, compute similarity with each expert:\n",
    "\n",
    "Tokenâ‚€ â€¢ Centroidâ‚€ = scoreâ‚€â‚€   â”€â”\n",
    "Tokenâ‚€ â€¢ Centroidâ‚ = scoreâ‚€â‚    â”‚ â† logits for Token 0\n",
    "Tokenâ‚€ â€¢ Centroidâ‚‚ = scoreâ‚€â‚‚    â”‚\n",
    "...                             â”€â”˜\n",
    "\n",
    "Result: [128 tokens Ã— 16 experts] = 128Ã—16 scores\n",
    "```\n",
    "\n",
    "```python\n",
    "            logits = logits + self.bias.to(logits.dtype)\n",
    "#                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Add bias to each expert's score\n",
    "#           .to(logits.dtype) ensures matching data types\n",
    "```\n",
    "\n",
    "### Step 4: Top-K Selection\n",
    "\n",
    "```python\n",
    "        topk_logits, topk_idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Select top_k highest values along last dimension\n",
    "#       \n",
    "#       topk_logits: [N, k] = [128, 8] â† the actual scores\n",
    "#       topk_idx:    [N, k] = [128, 8] â† which experts were selected\n",
    "```\n",
    "\n",
    "```\n",
    "Example for one token:\n",
    "logits = [0.1, 0.9, 0.3, 0.7, 0.2, 0.8, 0.4, 0.6, ...]  (16 values)\n",
    "                â†‘        â†‘        â†‘\n",
    "         torch.topk(logits, k=3) selects top 3:\n",
    "         \n",
    "topk_logits = [0.9, 0.8, 0.7]  â† highest scores\n",
    "topk_idx    = [1,   5,   3]    â† which expert indices\n",
    "```\n",
    "\n",
    "```python\n",
    "        gate = F.softmax(topk_logits, dim=-1, dtype=x.dtype)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Convert scores to probabilities (sum to 1)\n",
    "#       \n",
    "#       Example: [0.9, 0.8, 0.7] â†’ softmax â†’ [0.38, 0.33, 0.29]\n",
    "#                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#                                                   sums to 1.0\n",
    "```\n",
    "\n",
    "### Step 5: Expert Dispatch\n",
    "\n",
    "```python\n",
    "        routed_out = torch.zeros_like(x_flat)  # [N, D]\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Initialize output buffer with zeros\n",
    "```\n",
    "\n",
    "```python\n",
    "        for i in range(self.n_routed):  # Loop through 16 experts\n",
    "```\n",
    "\n",
    "```python\n",
    "            mask = (topk_idx == i)\n",
    "#           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Boolean mask: where was expert i selected?\n",
    "#           Shape: [N, k] = [128, 8]\n",
    "```\n",
    "\n",
    "```\n",
    "Example (expert i=3):\n",
    "topk_idx = [[1, 5, 3, 7, ...],    â† Token 0 selected experts\n",
    "            [3, 2, 8, 1, ...],    â† Token 1 selected experts\n",
    "            [0, 3, 5, 9, ...],    â† Token 2 selected experts\n",
    "            ...]\n",
    "\n",
    "mask = [[False, False, True, False, ...],   â† expert 3 at position 2\n",
    "        [True, False, False, False, ...],   â† expert 3 at position 0\n",
    "        [False, True, False, False, ...],   â† expert 3 at position 1\n",
    "        ...]\n",
    "```\n",
    "\n",
    "```python\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)\n",
    "#           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Find indices where mask is True\n",
    "#           \n",
    "#           row_idx: which tokens selected expert i\n",
    "#           which_k: at which position in top-k\n",
    "```\n",
    "\n",
    "```python\n",
    "            if row_idx.numel() == 0:\n",
    "                continue\n",
    "#           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Skip if no tokens selected this expert\n",
    "#           .numel() = number of elements\n",
    "```\n",
    "\n",
    "```python\n",
    "            exp_in = x_flat.index_select(0, row_idx)  # [Ti, D]\n",
    "#           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Select only the tokens that chose this expert\n",
    "#           \n",
    "#           index_select(dim, indices):\n",
    "#             - dim=0 means select along first dimension (rows)\n",
    "#             - row_idx are the row indices to select\n",
    "```\n",
    "\n",
    "```python\n",
    "            out = self.routed[i](exp_in)  # [Ti, D]\n",
    "#           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Pass selected tokens through expert i's FFN\n",
    "```\n",
    "\n",
    "```python\n",
    "            w = gate[row_idx, which_k].unsqueeze(-1)  # [Ti, 1]\n",
    "#           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Get the gating weights for these tokens\n",
    "#           \n",
    "#           gate[row_idx, which_k]: select specific elements\n",
    "#           .unsqueeze(-1): add dimension at end [Ti] â†’ [Ti, 1]\n",
    "#                          (needed for broadcasting)\n",
    "```\n",
    "\n",
    "```python\n",
    "            routed_out.index_add_(0, row_idx, out * w)\n",
    "#           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#           Add weighted output back to original positions\n",
    "#           \n",
    "#           index_add_(dim, indices, source):\n",
    "#             - In-place addition at specific indices\n",
    "#             - routed_out[row_idx] += out * w\n",
    "```\n",
    "\n",
    "```\n",
    "Visual of dispatch:\n",
    "                                    Expert 0\n",
    "Token 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n",
    "Token 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ FFN 0 â”‚â”€â”€â–¶ weighted output â†’ routed_out[0,5,...]\n",
    "Token 12 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â””â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                                    Expert 1\n",
    "Token 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n",
    "Token 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ FFN 1 â”‚â”€â”€â–¶ weighted output â†’ routed_out[1,3,...]\n",
    "Token 7 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â””â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "... (repeat for all 16 experts)\n",
    "```\n",
    "\n",
    "### Step 6: Combine Outputs\n",
    "\n",
    "```python\n",
    "        routed_out = routed_out.view(B, S, D)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Reshape back: [128, 1024] â†’ [2, 64, 1024]\n",
    "```\n",
    "\n",
    "```python\n",
    "        return x + shared_out + routed_out\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Residual connection: add everything together\n",
    "#       \n",
    "#       output = original_input + shared_experts + routed_experts\n",
    "```\n",
    "\n",
    "```\n",
    "Final combination:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                 â”‚\n",
    "â”‚   x (input) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚        â”‚                              â”‚         â”‚\n",
    "â”‚        â”œâ”€â”€â–¶ Shared Experts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚         â”‚\n",
    "â”‚        â”‚                             â”‚â”‚         â”‚\n",
    "â”‚        â””â”€â”€â–¶ Router + Routed Experts â”€â”´â”´â”€â”€â–¶ ADD â”€â”¼â”€â”€â–¶ output\n",
    "â”‚                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ 6. Update Bias Method\n",
    "\n",
    "```python\n",
    "    @torch.no_grad()\n",
    "#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#   Decorator: disable gradient computation\n",
    "#   (we don't want gradients here, just direct updates)\n",
    "    \n",
    "    def update_bias(self, x: torch.Tensor):\n",
    "```\n",
    "\n",
    "```python\n",
    "        N = x.shape[0] * x.shape[1]\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Total number of tokens = Batch Ã— Sequence\n",
    "```\n",
    "\n",
    "```python\n",
    "        logits = F.linear(x.reshape(-1, self.d_model), self.centroids) + self.bias\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Recalculate router logits (same as in forward)\n",
    "```\n",
    "\n",
    "```python\n",
    "        _, idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Get which experts were selected (we don't need the values)\n",
    "#       _ means \"discard this value\"\n",
    "```\n",
    "\n",
    "```python\n",
    "        counts = torch.bincount(idx.flatten(), minlength=self.n_routed).float()\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Count how many times each expert was selected\n",
    "#       \n",
    "#       idx.flatten(): convert [128, 8] â†’ [1024] (all selections)\n",
    "#       bincount: count occurrences of each value 0-15\n",
    "#       minlength: ensure output has 16 elements\n",
    "#       .float(): convert to float for math operations\n",
    "```\n",
    "\n",
    "```\n",
    "Example:\n",
    "idx.flatten() = [1, 5, 3, 7, 3, 2, 8, 1, ...]  (1024 values)\n",
    "\n",
    "bincount counts occurrences:\n",
    "Expert 0: appeared 50 times\n",
    "Expert 1: appeared 80 times\n",
    "Expert 2: appeared 45 times\n",
    "...\n",
    "Expert 15: appeared 70 times\n",
    "\n",
    "counts = [50, 80, 45, ..., 70]\n",
    "```\n",
    "\n",
    "```python\n",
    "        avg = counts.sum() / max(1, self.n_routed)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Calculate average load per expert\n",
    "#       \n",
    "#       Example: sum=1024 (total selections), n_routed=16\n",
    "#                avg = 1024 / 16 = 64 (ideal: each expert gets 64)\n",
    "```\n",
    "\n",
    "```python\n",
    "        violation = (avg - counts) / (avg + 1e-6)\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       How far is each expert from the average?\n",
    "#       \n",
    "#       If count < avg: violation > 0 (under-loaded, needs boost)\n",
    "#       If count > avg: violation < 0 (over-loaded, needs penalty)\n",
    "#       \n",
    "#       1e-6 prevents division by zero\n",
    "```\n",
    "\n",
    "```\n",
    "Example:\n",
    "avg = 64\n",
    "counts = [50, 80, 45, 70, ...]\n",
    "\n",
    "violation[0] = (64 - 50) / 64 = +0.22  (under-loaded, increase bias)\n",
    "violation[1] = (64 - 80) / 64 = -0.25  (over-loaded, decrease bias)\n",
    "```\n",
    "\n",
    "```python\n",
    "        self.bias.add_(self.bias_lr * torch.tanh(violation))\n",
    "#       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#       Update bias in-place\n",
    "#       \n",
    "#       torch.tanh(): squash to [-1, 1] range (prevents extreme updates)\n",
    "#       self.bias_lr: step size (0.01 = small steps)\n",
    "#       .add_(): in-place addition (modifies self.bias directly)\n",
    "```\n",
    "\n",
    "```\n",
    "tanh function:\n",
    "                1 â”¤         â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                  â”‚       â•±\n",
    "                0 â”¼â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                  â”‚   â•±\n",
    "               -1 â”¤â”€â”€â•¯\n",
    "                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                  -3   -1   0    1    3\n",
    "\n",
    "Squashes any value to range [-1, 1]\n",
    "Prevents explosive bias updates!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š 7. Testing the Model\n",
    "\n",
    "```python\n",
    "# Configuration\n",
    "d_model = 1024         # Each token is a 1024-dim vector\n",
    "n_routed_exp = 16      # 16 routed experts\n",
    "n_shared_exp = 2       # 2 shared experts\n",
    "top_k = 8              # Select 8 experts per token\n",
    "\n",
    "# Create model\n",
    "model = DeepSeekMoE(d_model, n_routed_exp, n_shared_exp, top_k)\n",
    "\n",
    "# Create random input\n",
    "batch_size_new = 2\n",
    "seq_len_new = 64\n",
    "random_input_new = torch.randn(batch_size_new, seq_len_new, d_model)\n",
    "#                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#                  Random tensor from normal distribution\n",
    "#                  Shape: [2, 64, 1024]\n",
    "\n",
    "# Forward pass\n",
    "output_new = model(random_input_new)\n",
    "\n",
    "print(\"New output shape:\", output_new.shape)\n",
    "# Output: torch.Size([2, 64, 1024])  â† Same shape as input!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Quick Reference: Key PyTorch Operations\n",
    "\n",
    "| Operation | What It Does | Example |\n",
    "|-----------|--------------|---------|\n",
    "| `tensor.shape` | Get dimensions | `[2, 64, 1024]` |\n",
    "| `tensor.reshape(-1, D)` | Flatten to 2D | `[128, 1024]` |\n",
    "| `tensor.view(B, S, D)` | Reshape (must be contiguous) | `[2, 64, 1024]` |\n",
    "| `F.linear(x, w)` | Matrix multiply: `x @ w.T` | `[N, D] @ [E, D].T â†’ [N, E]` |\n",
    "| `F.softmax(x, dim=-1)` | Normalize to probabilities | Sum to 1.0 |\n",
    "| `torch.topk(x, k)` | Get k largest values | Values and indices |\n",
    "| `tensor.index_select(0, idx)` | Select rows by index | Subset of tensor |\n",
    "| `tensor.index_add_(0, idx, src)` | Add to specific rows | In-place accumulate |\n",
    "| `torch.zeros_like(x)` | Zero tensor same shape as x | Initialization |\n",
    "| `torch.bincount(x)` | Count occurrences | Histogram |\n",
    "| `torch.tanh(x)` | Hyperbolic tangent | Squash to [-1, 1] |\n",
    "| `tensor.add_(x)` | In-place addition | Modify in place |\n",
    "| `@torch.no_grad()` | Disable gradients | For inference/manual updates |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ Summary\n",
    "\n",
    "The code implements a **Mixture of Experts** layer where:\n",
    "\n",
    "1. **Shared experts** always process all tokens\n",
    "2. **Router** calculates which routed experts each token should use\n",
    "3. **Top-k selection** picks the best experts for each token\n",
    "4. **Gating** weights the contribution of each selected expert\n",
    "5. **Dispatch** sends tokens to their selected experts\n",
    "6. **Combine** adds everything together with a residual connection\n",
    "7. **Bias update** keeps expert loads balanced over time\n",
    "\n",
    "This achieves **large model capacity** with **efficient computation**! ğŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
