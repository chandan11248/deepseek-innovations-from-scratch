{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a813b6",
   "metadata": {},
   "source": [
    "# Multi-Token Prediction (MTP) - DeepSeek's Approach\n",
    "\n",
    "## What is Multi-Token Prediction?\n",
    "\n",
    "**Traditional LLMs:** Predict **ONE token at a time** (slow!)\n",
    "\n",
    "**Multi-Token Prediction:** Predict **MULTIPLE tokens at once** (fast!)\n",
    "\n",
    "```\n",
    "Traditional (Next-Token Prediction):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                                                                 \u2502\n",
    "\u2502   \"The cat sat on the\" \u2500\u2500\u25b6 Model \u2500\u2500\u25b6 \"mat\"                      \u2502\n",
    "\u2502   \"The cat sat on the mat\" \u2500\u2500\u25b6 Model \u2500\u2500\u25b6 \"and\"                  \u2502\n",
    "\u2502   \"The cat sat on the mat and\" \u2500\u2500\u25b6 Model \u2500\u2500\u25b6 \"slept\"            \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502   3 tokens = 3 forward passes (SLOW!)                           \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Multi-Token Prediction (MTP):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                                                                 \u2502\n",
    "\u2502   \"The cat sat on the\" \u2500\u2500\u25b6 Model \u2500\u2500\u25b6 \"mat\", \"and\", \"slept\"      \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502   3 tokens = 1 forward pass (FAST!)                             \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why Does This Matter?\n",
    "\n",
    "```\n",
    "The Bottleneck Problem:\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "In autoregressive generation, each token depends on ALL previous tokens.\n",
    "\n",
    "Token 1 \u2500\u2500\u25b6 Token 2 \u2500\u2500\u25b6 Token 3 \u2500\u2500\u25b6 Token 4 \u2500\u2500\u25b6 ...\n",
    "   \u2502           \u2502           \u2502           \u2502\n",
    "   \u25bc           \u25bc           \u25bc           \u25bc\n",
    "Forward     Forward     Forward     Forward\n",
    "Pass 1      Pass 2      Pass 3      Pass 4\n",
    "\n",
    "Problem: Sequential dependency = Can't parallelize during inference!\n",
    "```\n",
    "\n",
    "**MTP Solution:** Train the model to predict multiple future tokens simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## How DeepSeek Implements MTP\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "                              Input Tokens\n",
    "                                   \u2502\n",
    "                                   \u25bc\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502                          \u2502\n",
    "                    \u2502     Main Transformer     \u2502\n",
    "                    \u2502      (Shared Trunk)      \u2502\n",
    "                    \u2502                          \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                 \u2502\n",
    "                      Hidden States [h\u2081, h\u2082, h\u2083, ...]\n",
    "                                 \u2502\n",
    "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "            \u2502                    \u2502                    \u2502\n",
    "            \u25bc                    \u25bc                    \u25bc\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502   MTP Head   \u2502     \u2502   MTP Head   \u2502     \u2502   MTP Head   \u2502\n",
    "    \u2502   (k = 1)    \u2502     \u2502   (k = 2)    \u2502     \u2502   (k = 3)    \u2502\n",
    "    \u2502  Next Token  \u2502     \u2502  +2 Token    \u2502     \u2502  +3 Token    \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502                    \u2502                    \u2502\n",
    "           \u25bc                    \u25bc                    \u25bc\n",
    "        Token\u2081               Token\u2082               Token\u2083\n",
    "      (position t+1)       (position t+2)       (position t+3)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "```\n",
    "1. Shared Transformer Trunk:\n",
    "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502  Same backbone processes input once     \u2502\n",
    "   \u2502  Produces rich hidden representations   \u2502\n",
    "   \u2502  Most computation happens here          \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "2. Multiple Prediction Heads:\n",
    "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502  Lightweight heads (small MLPs)         \u2502\n",
    "   \u2502  Each head predicts a different future  \u2502\n",
    "   \u2502  position: t+1, t+2, t+3, ...           \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## DeepSeek's MTP Module Design\n",
    "\n",
    "```\n",
    "For each prediction depth k:\n",
    "\n",
    "    Hidden State (from transformer)\n",
    "           \u2502\n",
    "           \u25bc\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502  Embedding  \u2502  \u25c4\u2500\u2500 Previous prediction's embedding\n",
    "    \u2502   Lookup    \u2502      (for k > 1, chain predictions)\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502   Concat    \u2502  \u25c4\u2500\u2500 Combine hidden state + embedding\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502  MTP Block  \u2502  \u25c4\u2500\u2500 Small transformer layer\n",
    "    \u2502  (1 layer)  \u2502      (self-attention + FFN)\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502   Output    \u2502  \u25c4\u2500\u2500 Project to vocabulary\n",
    "    \u2502   Head      \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "      Prediction k\n",
    "```\n",
    "\n",
    "### The Chaining Mechanism\n",
    "\n",
    "```\n",
    "How predictions flow:\n",
    "\n",
    "Step 1: Main Model produces hidden states\n",
    "        h = Transformer(input_tokens)\n",
    "\n",
    "Step 2: Head 1 predicts token at t+1\n",
    "        pred\u2081 = Head\u2081(h)\n",
    "\n",
    "Step 3: Head 2 uses pred\u2081's embedding + h to predict t+2\n",
    "        pred\u2082 = Head\u2082(concat(h, embed(pred\u2081)))\n",
    "\n",
    "Step 4: Head 3 uses pred\u2082's embedding + h to predict t+3\n",
    "        pred\u2083 = Head\u2083(concat(h, embed(pred\u2082)))\n",
    "\n",
    "... and so on for k prediction depths\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Training vs Inference\n",
    "\n",
    "### During Training\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                         TRAINING                                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Input: \"The cat sat on the mat and slept\"                      \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  For position \"the\":                                            \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n",
    "\u2502  \u2502  Head 1 target: \"mat\"      (t+1)        \u2502                    \u2502\n",
    "\u2502  \u2502  Head 2 target: \"and\"      (t+2)        \u2502                    \u2502\n",
    "\u2502  \u2502  Head 3 target: \"slept\"    (t+3)        \u2502                    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Loss = Loss\u2081 + Loss\u2082 + Loss\u2083                                   \u2502\n",
    "\u2502         (weighted sum of all prediction losses)                 \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### During Inference (Speculative Decoding)\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                        INFERENCE                                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Step 1: Generate k draft tokens using MTP heads                \u2502\n",
    "\u2502          draft = [token\u2081, token\u2082, token\u2083, ...]                  \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Step 2: Verify ALL drafts in ONE forward pass                  \u2502\n",
    "\u2502          verified = MainModel.verify(draft)                     \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Step 3: Accept correct predictions, reject wrong ones          \u2502\n",
    "\u2502          If token\u2081 \u2713, token\u2082 \u2713, token\u2083 \u2717                        \u2502\n",
    "\u2502          Accept: token\u2081, token\u2082                                 \u2502\n",
    "\u2502          Regenerate from token\u2083                                 \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why MTP Makes Inference Faster\n",
    "\n",
    "### The Speedup Intuition\n",
    "\n",
    "```\n",
    "Traditional Autoregressive:\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Generate 12 tokens = 12 sequential forward passes\n",
    "\n",
    "Pass 1 \u2500\u2500\u25b6 Pass 2 \u2500\u2500\u25b6 Pass 3 \u2500\u2500\u25b6 ... \u2500\u2500\u25b6 Pass 12\n",
    "  \u2502          \u2502          \u2502                   \u2502\n",
    "  \u25bc          \u25bc          \u25bc                   \u25bc\n",
    " T1         T2         T3         ...      T12\n",
    "\n",
    "Time = 12 \u00d7 (forward pass time)\n",
    "\n",
    "\n",
    "With MTP (k=4 prediction depth):\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Generate 12 tokens \u2248 3-4 forward passes (with verification)\n",
    "\n",
    "Pass 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 Pass 2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 Pass 3\n",
    "  \u2502                      \u2502                      \u2502\n",
    "  \u25bc                      \u25bc                      \u25bc\n",
    "T1,T2,T3,T4           T5,T6,T7,T8           T9,T10,T11,T12\n",
    "(draft+verify)        (draft+verify)        (draft+verify)\n",
    "\n",
    "Time \u2248 3-4 \u00d7 (forward pass time)\n",
    "\n",
    "Speedup: ~3-4x faster!\n",
    "```\n",
    "\n",
    "### Acceptance Rate\n",
    "\n",
    "```\n",
    "The key metric: How often are draft tokens correct?\n",
    "\n",
    "High Acceptance Rate (good):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Draft: [mat, and, slept, peacefully]   \u2502\n",
    "\u2502  Verify: [\u2713,   \u2713,   \u2713,     \u2713]           \u2502\n",
    "\u2502  Accept ALL 4 tokens!                   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Low Acceptance Rate (less speedup):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Draft: [mat, or,  jumped, quickly]     \u2502\n",
    "\u2502  Verify: [\u2713,   \u2717,   -,      -]          \u2502\n",
    "\u2502  Accept only 1 token, regenerate rest   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "DeepSeek reports: ~85-90% acceptance rate for greedy decoding\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of MTP\n",
    "\n",
    "### 1. Faster Inference\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Tokens per Second Comparison:                                  \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Standard:    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 50 tok/s                         \u2502\n",
    "\u2502  With MTP:    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 150 tok/s    \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  ~2-3x speedup in practice!                                     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### 2. Better Representations (Training Benefit)\n",
    "\n",
    "```\n",
    "Why training with MTP helps:\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Predicting multiple tokens forces the model to:\n",
    "\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502  1. Plan ahead (not just next token)    \u2502\n",
    "  \u2502  2. Learn longer-range dependencies     \u2502\n",
    "  \u2502  3. Build richer hidden representations \u2502\n",
    "  \u2502  4. Better understand context           \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Result: Even single-token prediction improves!\n",
    "```\n",
    "\n",
    "### 3. Memory Efficiency\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  KV Cache Savings:                                              \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Traditional: Update cache 1 token at a time                    \u2502\n",
    "\u2502  MTP: Update cache for multiple tokens at once                  \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Fewer memory operations = Less memory bandwidth used           \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## DeepSeek's Specific Implementation Details\n",
    "\n",
    "### MTP Head Structure\n",
    "\n",
    "```python\n",
    "# Simplified structure of each MTP head:\n",
    "\n",
    "class MTPHead:\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        self.embed = Embedding(vocab_size, d_model)\n",
    "        self.proj = Linear(2 * d_model, d_model)  # Concat input\n",
    "        self.transformer_block = TransformerBlock(d_model)\n",
    "        self.output = Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, hidden_state, prev_token_embed):\n",
    "        # Combine hidden state with previous prediction\n",
    "        x = concat(hidden_state, prev_token_embed)\n",
    "        x = self.proj(x)\n",
    "        x = self.transformer_block(x)\n",
    "        logits = self.output(x)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "```\n",
    "Total Loss = \u03bb\u2080\u00b7L\u2080 + \u03bb\u2081\u00b7L\u2081 + \u03bb\u2082\u00b7L\u2082 + ... + \u03bb\u2096\u00b7L\u2096\n",
    "\n",
    "Where:\n",
    "  L\u2080 = Main next-token prediction loss\n",
    "  L\u2081 = MTP head 1 loss (t+1)\n",
    "  L\u2082 = MTP head 2 loss (t+2)\n",
    "  ...\n",
    "  \u03bb\u1d62 = Weight for each head (often decreasing)\n",
    "\n",
    "DeepSeek uses: \u03bb = 1.0 for all heads (equal weighting)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Summary\n",
    "\n",
    "![Multi-Token Prediction Architecture](assets/mtp_architecture.png)\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              Multi-Token Prediction Pipeline                    \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  TRAINING:                                                      \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                                      \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Input \u2500\u2500\u25b6 Transformer \u2500\u2500\u252c\u2500\u2500\u25b6 Head\u2081 \u2500\u2500\u25b6 Loss (t+1)              \u2502\n",
    "\u2502                          \u251c\u2500\u2500\u25b6 Head\u2082 \u2500\u2500\u25b6 Loss (t+2)              \u2502\n",
    "\u2502                          \u251c\u2500\u2500\u25b6 Head\u2083 \u2500\u2500\u25b6 Loss (t+3)              \u2502\n",
    "\u2502                          \u2514\u2500\u2500\u25b6 Head\u2084 \u2500\u2500\u25b6 Loss (t+4)              \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  All heads trained jointly, shared trunk learns better!         \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  INFERENCE (Speculative Decoding):                              \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                              \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n",
    "\u2502  \u2502  Draft  \u2502 \u2500\u2500\u25b6 \u2502    Verify    \u2502 \u2500\u2500\u25b6 \u2502   Accept   \u2502            \u2502\n",
    "\u2502  \u2502 k tokens\u2502     \u2502 (1 fwd pass) \u2502     \u2502 or Reject  \u2502            \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Accepted tokens: Skip generation, move forward fast!           \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Aspect | Traditional | With MTP |\n",
    "|--------|-------------|----------|\n",
    "| **Tokens per forward pass** | 1 | k (multiple) |\n",
    "| **Inference speed** | Baseline | 2-3x faster |\n",
    "| **Training signal** | Next token only | Multiple future tokens |\n",
    "| **Representation quality** | Good | Better (plans ahead) |\n",
    "| **Memory efficiency** | Standard | Improved (batch KV updates) |\n",
    "| **Complexity** | Simple | Slightly more complex |\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Works: The Intuition\n",
    "\n",
    "```\n",
    "Think of it like writing:\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Slow writer (traditional):\n",
    "  \"The\" \u2500\u2500\u25b6 think \u2500\u2500\u25b6 \"cat\" \u2500\u2500\u25b6 think \u2500\u2500\u25b6 \"sat\" \u2500\u2500\u25b6 think \u2500\u2500\u25b6 ...\n",
    "\n",
    "Fast writer (MTP):\n",
    "  \"The\" \u2500\u2500\u25b6 think \u2500\u2500\u25b6 \"cat sat on\" \u2500\u2500\u25b6 verify \u2500\u2500\u25b6 continue...\n",
    "\n",
    "The fast writer:\n",
    "  1. Has a plan in mind (multiple tokens)\n",
    "  2. Writes in chunks\n",
    "  3. Only pauses to verify occasionally\n",
    "\n",
    "Same idea for LLMs:\n",
    "  - MTP heads draft multiple tokens quickly\n",
    "  - Main model verifies in one pass\n",
    "  - Accept correct predictions, retry wrong ones\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Multi-Token Prediction (MTP) as implemented by DeepSeek:\n",
    "\n",
    "1. **Trains multiple prediction heads** to forecast future tokens\n",
    "2. **Uses speculative decoding** during inference\n",
    "3. **Achieves 2-3x speedup** with high acceptance rates\n",
    "4. **Improves model quality** by learning longer-range dependencies\n",
    "5. **Reduces memory operations** by batching KV cache updates\n",
    "\n",
    "This is a key technique that makes DeepSeek-V3 both **fast** and **capable**!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}