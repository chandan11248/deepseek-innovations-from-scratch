{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a813b6",
   "metadata": {},
   "source": [
    "# Multi-Token Prediction (MTP) - DeepSeek's Approach\n",
    "\n",
    "## What is Multi-Token Prediction?\n",
    "\n",
    "**Traditional LLMs:** Predict **ONE token at a time** (slow!)\n",
    "\n",
    "**Multi-Token Prediction:** Predict **MULTIPLE tokens at once** (fast!)\n",
    "\n",
    "```\n",
    "Traditional (Next-Token Prediction):\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                                                                 │\n",
    "│   \"The cat sat on the\" ──▶ Model ──▶ \"mat\"                      │\n",
    "│   \"The cat sat on the mat\" ──▶ Model ──▶ \"and\"                  │\n",
    "│   \"The cat sat on the mat and\" ──▶ Model ──▶ \"slept\"            │\n",
    "│                                                                 │\n",
    "│   3 tokens = 3 forward passes (SLOW!)                           │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Multi-Token Prediction (MTP):\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                                                                 │\n",
    "│   \"The cat sat on the\" ──▶ Model ──▶ \"mat\", \"and\", \"slept\"      │\n",
    "│                                                                 │\n",
    "│   3 tokens = 1 forward pass (FAST!)                             │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why Does This Matter?\n",
    "\n",
    "```\n",
    "The Bottleneck Problem:\n",
    "═══════════════════════\n",
    "\n",
    "In autoregressive generation, each token depends on ALL previous tokens.\n",
    "\n",
    "Token 1 ──▶ Token 2 ──▶ Token 3 ──▶ Token 4 ──▶ ...\n",
    "   │           │           │           │\n",
    "   ▼           ▼           ▼           ▼\n",
    "Forward     Forward     Forward     Forward\n",
    "Pass 1      Pass 2      Pass 3      Pass 4\n",
    "\n",
    "Problem: Sequential dependency = Can't parallelize during inference!\n",
    "```\n",
    "\n",
    "**MTP Solution:** Train the model to predict multiple future tokens simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## How DeepSeek Implements MTP\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "                              Input Tokens\n",
    "                                   │\n",
    "                                   ▼\n",
    "                    ┌──────────────────────────┐\n",
    "                    │                          │\n",
    "                    │     Main Transformer     │\n",
    "                    │      (Shared Trunk)      │\n",
    "                    │                          │\n",
    "                    └────────────┬─────────────┘\n",
    "                                 │\n",
    "                      Hidden States [h₁, h₂, h₃, ...]\n",
    "                                 │\n",
    "            ┌────────────────────┼────────────────────┐\n",
    "            │                    │                    │\n",
    "            ▼                    ▼                    ▼\n",
    "    ┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "    │   MTP Head   │     │   MTP Head   │     │   MTP Head   │\n",
    "    │   (k = 1)    │     │   (k = 2)    │     │   (k = 3)    │\n",
    "    │  Next Token  │     │  +2 Token    │     │  +3 Token    │\n",
    "    └──────┬───────┘     └──────┬───────┘     └──────┬───────┘\n",
    "           │                    │                    │\n",
    "           ▼                    ▼                    ▼\n",
    "        Token₁               Token₂               Token₃\n",
    "      (position t+1)       (position t+2)       (position t+3)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "```\n",
    "1. Shared Transformer Trunk:\n",
    "   ┌─────────────────────────────────────────┐\n",
    "   │  Same backbone processes input once     │\n",
    "   │  Produces rich hidden representations   │\n",
    "   │  Most computation happens here          │\n",
    "   └─────────────────────────────────────────┘\n",
    "\n",
    "2. Multiple Prediction Heads:\n",
    "   ┌─────────────────────────────────────────┐\n",
    "   │  Lightweight heads (small MLPs)         │\n",
    "   │  Each head predicts a different future  │\n",
    "   │  position: t+1, t+2, t+3, ...           │\n",
    "   └─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## DeepSeek's MTP Module Design\n",
    "\n",
    "```\n",
    "For each prediction depth k:\n",
    "\n",
    "    Hidden State (from transformer)\n",
    "           │\n",
    "           ▼\n",
    "    ┌─────────────┐\n",
    "    │  Embedding  │  ◄── Previous prediction's embedding\n",
    "    │   Lookup    │      (for k > 1, chain predictions)\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "           ▼\n",
    "    ┌─────────────┐\n",
    "    │   Concat    │  ◄── Combine hidden state + embedding\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "           ▼\n",
    "    ┌─────────────┐\n",
    "    │  MTP Block  │  ◄── Small transformer layer\n",
    "    │  (1 layer)  │      (self-attention + FFN)\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "           ▼\n",
    "    ┌─────────────┐\n",
    "    │   Output    │  ◄── Project to vocabulary\n",
    "    │   Head      │\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "           ▼\n",
    "      Prediction k\n",
    "```\n",
    "\n",
    "### The Chaining Mechanism\n",
    "\n",
    "```\n",
    "How predictions flow:\n",
    "\n",
    "Step 1: Main Model produces hidden states\n",
    "        h = Transformer(input_tokens)\n",
    "\n",
    "Step 2: Head 1 predicts token at t+1\n",
    "        pred₁ = Head₁(h)\n",
    "\n",
    "Step 3: Head 2 uses pred₁'s embedding + h to predict t+2\n",
    "        pred₂ = Head₂(concat(h, embed(pred₁)))\n",
    "\n",
    "Step 4: Head 3 uses pred₂'s embedding + h to predict t+3\n",
    "        pred₃ = Head₃(concat(h, embed(pred₂)))\n",
    "\n",
    "... and so on for k prediction depths\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Training vs Inference\n",
    "\n",
    "### During Training\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         TRAINING                                │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  Input: \"The cat sat on the mat and slept\"                      │\n",
    "│                                                                 │\n",
    "│  For position \"the\":                                            │\n",
    "│  ┌─────────────────────────────────────────┐                    │\n",
    "│  │  Head 1 target: \"mat\"      (t+1)        │                    │\n",
    "│  │  Head 2 target: \"and\"      (t+2)        │                    │\n",
    "│  │  Head 3 target: \"slept\"    (t+3)        │                    │\n",
    "│  └─────────────────────────────────────────┘                    │\n",
    "│                                                                 │\n",
    "│  Loss = Loss₁ + Loss₂ + Loss₃                                   │\n",
    "│         (weighted sum of all prediction losses)                 │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### During Inference (Speculative Decoding)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                        INFERENCE                                │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  Step 1: Generate k draft tokens using MTP heads                │\n",
    "│          draft = [token₁, token₂, token₃, ...]                  │\n",
    "│                                                                 │\n",
    "│  Step 2: Verify ALL drafts in ONE forward pass                  │\n",
    "│          verified = MainModel.verify(draft)                     │\n",
    "│                                                                 │\n",
    "│  Step 3: Accept correct predictions, reject wrong ones          │\n",
    "│          If token₁ ✓, token₂ ✓, token₃ ✗                        │\n",
    "│          Accept: token₁, token₂                                 │\n",
    "│          Regenerate from token₃                                 │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why MTP Makes Inference Faster\n",
    "\n",
    "### The Speedup Intuition\n",
    "\n",
    "```\n",
    "Traditional Autoregressive:\n",
    "═══════════════════════════\n",
    "\n",
    "Generate 12 tokens = 12 sequential forward passes\n",
    "\n",
    "Pass 1 ──▶ Pass 2 ──▶ Pass 3 ──▶ ... ──▶ Pass 12\n",
    "  │          │          │                   │\n",
    "  ▼          ▼          ▼                   ▼\n",
    " T1         T2         T3         ...      T12\n",
    "\n",
    "Time = 12 × (forward pass time)\n",
    "\n",
    "\n",
    "With MTP (k=4 prediction depth):\n",
    "════════════════════════════════\n",
    "\n",
    "Generate 12 tokens ≈ 3-4 forward passes (with verification)\n",
    "\n",
    "Pass 1 ──────────────▶ Pass 2 ──────────────▶ Pass 3\n",
    "  │                      │                      │\n",
    "  ▼                      ▼                      ▼\n",
    "T1,T2,T3,T4           T5,T6,T7,T8           T9,T10,T11,T12\n",
    "(draft+verify)        (draft+verify)        (draft+verify)\n",
    "\n",
    "Time ≈ 3-4 × (forward pass time)\n",
    "\n",
    "Speedup: ~3-4x faster!\n",
    "```\n",
    "\n",
    "### Acceptance Rate\n",
    "\n",
    "```\n",
    "The key metric: How often are draft tokens correct?\n",
    "\n",
    "High Acceptance Rate (good):\n",
    "┌─────────────────────────────────────────┐\n",
    "│  Draft: [mat, and, slept, peacefully]   │\n",
    "│  Verify: [✓,   ✓,   ✓,     ✓]           │\n",
    "│  Accept ALL 4 tokens!                   │\n",
    "└─────────────────────────────────────────┘\n",
    "\n",
    "Low Acceptance Rate (less speedup):\n",
    "┌─────────────────────────────────────────┐\n",
    "│  Draft: [mat, or,  jumped, quickly]     │\n",
    "│  Verify: [✓,   ✗,   -,      -]          │\n",
    "│  Accept only 1 token, regenerate rest   │\n",
    "└─────────────────────────────────────────┘\n",
    "\n",
    "DeepSeek reports: ~85-90% acceptance rate for greedy decoding\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of MTP\n",
    "\n",
    "### 1. Faster Inference\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  Tokens per Second Comparison:                                  │\n",
    "│                                                                 │\n",
    "│  Standard:    ████████████████ 50 tok/s                         │\n",
    "│  With MTP:    ████████████████████████████████████ 150 tok/s    │\n",
    "│                                                                 │\n",
    "│  ~2-3x speedup in practice!                                     │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 2. Better Representations (Training Benefit)\n",
    "\n",
    "```\n",
    "Why training with MTP helps:\n",
    "════════════════════════════\n",
    "\n",
    "Predicting multiple tokens forces the model to:\n",
    "\n",
    "  ┌─────────────────────────────────────────┐\n",
    "  │  1. Plan ahead (not just next token)    │\n",
    "  │  2. Learn longer-range dependencies     │\n",
    "  │  3. Build richer hidden representations │\n",
    "  │  4. Better understand context           │\n",
    "  └─────────────────────────────────────────┘\n",
    "\n",
    "Result: Even single-token prediction improves!\n",
    "```\n",
    "\n",
    "### 3. Memory Efficiency\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  KV Cache Savings:                                              │\n",
    "│                                                                 │\n",
    "│  Traditional: Update cache 1 token at a time                    │\n",
    "│  MTP: Update cache for multiple tokens at once                  │\n",
    "│                                                                 │\n",
    "│  Fewer memory operations = Less memory bandwidth used           │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## DeepSeek's Specific Implementation Details\n",
    "\n",
    "### MTP Head Structure\n",
    "\n",
    "```python\n",
    "# Simplified structure of each MTP head:\n",
    "\n",
    "class MTPHead:\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        self.embed = Embedding(vocab_size, d_model)\n",
    "        self.proj = Linear(2 * d_model, d_model)  # Concat input\n",
    "        self.transformer_block = TransformerBlock(d_model)\n",
    "        self.output = Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, hidden_state, prev_token_embed):\n",
    "        # Combine hidden state with previous prediction\n",
    "        x = concat(hidden_state, prev_token_embed)\n",
    "        x = self.proj(x)\n",
    "        x = self.transformer_block(x)\n",
    "        logits = self.output(x)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "```\n",
    "Total Loss = λ₀·L₀ + λ₁·L₁ + λ₂·L₂ + ... + λₖ·Lₖ\n",
    "\n",
    "Where:\n",
    "  L₀ = Main next-token prediction loss\n",
    "  L₁ = MTP head 1 loss (t+1)\n",
    "  L₂ = MTP head 2 loss (t+2)\n",
    "  ...\n",
    "  λᵢ = Weight for each head (often decreasing)\n",
    "\n",
    "DeepSeek uses: λ = 1.0 for all heads (equal weighting)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│              Multi-Token Prediction Pipeline                    │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  TRAINING:                                                      │\n",
    "│  ─────────                                                      │\n",
    "│                                                                 │\n",
    "│  Input ──▶ Transformer ──┬──▶ Head₁ ──▶ Loss (t+1)              │\n",
    "│                          ├──▶ Head₂ ──▶ Loss (t+2)              │\n",
    "│                          ├──▶ Head₃ ──▶ Loss (t+3)              │\n",
    "│                          └──▶ Head₄ ──▶ Loss (t+4)              │\n",
    "│                                                                 │\n",
    "│  All heads trained jointly, shared trunk learns better!         │\n",
    "│                                                                 │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  INFERENCE (Speculative Decoding):                              │\n",
    "│  ─────────────────────────────────                              │\n",
    "│                                                                 │\n",
    "│  ┌─────────┐     ┌──────────────┐     ┌────────────┐            │\n",
    "│  │  Draft  │ ──▶ │    Verify    │ ──▶ │   Accept   │            │\n",
    "│  │ k tokens│     │ (1 fwd pass) │     │ or Reject  │            │\n",
    "│  └─────────┘     └──────────────┘     └────────────┘            │\n",
    "│                                                                 │\n",
    "│  Accepted tokens: Skip generation, move forward fast!           │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Aspect | Traditional | With MTP |\n",
    "|--------|-------------|----------|\n",
    "| **Tokens per forward pass** | 1 | k (multiple) |\n",
    "| **Inference speed** | Baseline | 2-3x faster |\n",
    "| **Training signal** | Next token only | Multiple future tokens |\n",
    "| **Representation quality** | Good | Better (plans ahead) |\n",
    "| **Memory efficiency** | Standard | Improved (batch KV updates) |\n",
    "| **Complexity** | Simple | Slightly more complex |\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Works: The Intuition\n",
    "\n",
    "```\n",
    "Think of it like writing:\n",
    "═════════════════════════\n",
    "\n",
    "Slow writer (traditional):\n",
    "  \"The\" ──▶ think ──▶ \"cat\" ──▶ think ──▶ \"sat\" ──▶ think ──▶ ...\n",
    "\n",
    "Fast writer (MTP):\n",
    "  \"The\" ──▶ think ──▶ \"cat sat on\" ──▶ verify ──▶ continue...\n",
    "\n",
    "The fast writer:\n",
    "  1. Has a plan in mind (multiple tokens)\n",
    "  2. Writes in chunks\n",
    "  3. Only pauses to verify occasionally\n",
    "\n",
    "Same idea for LLMs:\n",
    "  - MTP heads draft multiple tokens quickly\n",
    "  - Main model verifies in one pass\n",
    "  - Accept correct predictions, retry wrong ones\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Multi-Token Prediction (MTP) as implemented by DeepSeek:\n",
    "\n",
    "1. **Trains multiple prediction heads** to forecast future tokens\n",
    "2. **Uses speculative decoding** during inference\n",
    "3. **Achieves 2-3x speedup** with high acceptance rates\n",
    "4. **Improves model quality** by learning longer-range dependencies\n",
    "5. **Reduces memory operations** by batching KV cache updates\n",
    "\n",
    "This is a key technique that makes DeepSeek-V3 both **fast** and **capable**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db4b9f",
   "metadata": {},
   "source": [
    "## Code Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4ff8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22c88bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps:float=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "    def forward(self,x):\n",
    "        rms=torch.sqrt(x.pow(2).mean(dim=-1,keepdim=True+self.eps))\n",
    "        return x/rms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7096f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMTP(nn.Module):\n",
    "    def __init__(self,d_model:int,vocab_size:int,num_heads:int=3,n_head:int=2):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_heads=num_heads\n",
    "        self.n_head=n_head\n",
    "\n",
    "        #shared modules\n",
    "        self.RMSNorm=RMSNorm(d_model=self.d_model)\n",
    "        self.embed=nn.Embedding(vocab_size,d_model)\n",
    "        self.unembed=nn.Linear(d_model,vocab_size,bias=False)\n",
    "        # shared weights between embed and unembed \n",
    "        self.unembed.weight=self.embed.weight\n",
    "        # one projection + transformer per head \n",
    "        self.projections=nn.ModuleList([nn.Linear(2*d_model,d_model) for _ in range(num_heads)])\n",
    "\n",
    "        self.tranformer=nn.ModuleList([nn.TransformerEncoderLayer(d_model=d_model,nhead=n_head ) for _ in range(num_heads)])\n",
    "\n",
    "\n",
    "    ''' token_ids: (batch, seq_len) integer IDs of your input tokens init_hidden: optional (batch, seq_len, d_model) base hidden states;\n",
    "    # if None, uses token embeddings as initial '''\n",
    "\n",
    "    '''Returns:\n",
    "        logits_out: Tensor of shape (batch, T-D, D, vocab_size), where T=sea_len and D=num_heads'''\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor, init_hidden: torch.Tensor = None):\n",
    "        B, T = token_ids.shape\n",
    "        device = token_ids.device\n",
    "\n",
    "        # token embeddings: (B, T, d_model)\n",
    "        embeds = self.embed(token_ids)\n",
    "        \n",
    "        # base hidden states\n",
    "        if init_hidden is None:\n",
    "            h0_seq = embeds  # use embeddings as base hidden\n",
    "        else:\n",
    "            h0_seq = init_hidden  # user-provided base states\n",
    "        \n",
    "        outputs = []  # will hold (B, num_heads, vocab_size) for each position i\n",
    "        \n",
    "        # slide over positions where i + num_heads < T\n",
    "        max_i = T - self.num_heads - 1\n",
    "        \n",
    "        for i in range(0, max_i + 1):\n",
    "            # previous hidden for depth 0 at pos i\n",
    "            h_prev = h0_seq[:, i, :]  # (B, d_model)\n",
    "            \n",
    "            # collect logits for all k at this position i\n",
    "            logits_k = []\n",
    "            \n",
    "            for k in range(self.num_heads):\n",
    "                # future token embed at pos i + (k+1)\n",
    "                future_pos = i + (k + 1)\n",
    "                tok_embed = embeds[:, future_pos, :]  # (B, d_model)\n",
    "                \n",
    "                # concatenate hidden state with future token embedding\n",
    "                x = torch.cat([h_prev, tok_embed], dim=-1)  # (B, 2*d_model)\n",
    "                \n",
    "                # project down to d_model\n",
    "                x = self.projections[k](x)  # (B, d_model)\n",
    "                \n",
    "                # apply RMSNorm\n",
    "                x = self.RMSNorm(x)\n",
    "                \n",
    "                # pass through transformer block (adding batch dimension for transformer)\n",
    "                x = x.unsqueeze(1)  # (B, 1, d_model)\n",
    "                x = self.tranformer[k](x)  # (B, 1, d_model)\n",
    "                x = x.squeeze(1)  # (B, d_model)\n",
    "                \n",
    "                # project to vocabulary size\n",
    "                logits = self.unembed(x)  # (B, vocab_size)\n",
    "                logits_k.append(logits)\n",
    "                \n",
    "                # update h_prev for next prediction head (chain predictions)\n",
    "                h_prev = x\n",
    "            \n",
    "            # stack all predictions for this position: (B, num_heads, vocab_size)\n",
    "            logits_k = torch.stack(logits_k, dim=1)\n",
    "            outputs.append(logits_k)\n",
    "        \n",
    "        # stack all positions: (B, T-num_heads-1, num_heads, vocab_size)\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbde3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755443e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
