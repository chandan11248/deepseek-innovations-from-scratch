{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e25820",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Imports PyTorch and necessary modules for building the Mixture of Experts model. Sets up torch.manual_seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d7ef828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.random.manual_seed(seed) -> torch._C.Generator>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbbf20",
   "metadata": {},
   "source": [
    "## Download Training Data\n",
    "Downloads the input text file from GitHub that will be used for training the MoE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae0f6f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  1937k      0 --:--:-- --:--:-- --:--:-- 1934k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d1c4d",
   "metadata": {},
   "source": [
    "## Define Expert Module\n",
    "Defines an `Expert` class - a simple MLP (Multi-Layer Perceptron) that serves as one expert in the MoE architecture. Each expert consists of two linear layers with ReLU activation and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e57e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert module\n",
    "'''An MLP is a simple linear layer followed by a non-linearity i.e. each Expert'''\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233d4d1",
   "metadata": {},
   "source": [
    "## Understanding Gating Mechanism - Setup\n",
    "Sets up a simple example to demonstrate how the gating/routing mechanism works. Creates sample multi-head attention output and a linear layer to produce logits for expert selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4501915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0123,  0.3042,  0.4986,  0.3198],\n",
      "         [ 0.1164,  0.1188,  0.2375,  0.1165],\n",
      "         [ 0.3512,  0.1809,  0.1322,  0.2058],\n",
      "         [-0.2247,  0.2521,  0.6194,  0.2505]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Understanding how gating works\n",
    "num_experts = 4\n",
    "top_k=2\n",
    "n_embed=8\n",
    "\n",
    "#Example multi-head attention output for a simple illustrative example, consider n_embed=32, context_length=4\n",
    "mh_output = torch. rand(1, 4, n_embed)\n",
    "topkgate_linear = nn.Linear(n_embed, num_experts) # nn.Linear(32, 4)\n",
    "logits = topkgate_linear (mh_output)\n",
    "print (logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf1aee",
   "metadata": {},
   "source": [
    "## Select Top-K Experts\n",
    "Demonstrates selecting the top-k (top 2) experts with highest logits for each token. Returns both the logit values and the indices of selected experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d97b4ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4986, 0.3198],\n",
       "          [0.2375, 0.1188],\n",
       "          [0.3512, 0.2058],\n",
       "          [0.6194, 0.2521]]], grad_fn=<TopkBackward0>),\n",
       " tensor([[[2, 3],\n",
       "          [2, 1],\n",
       "          [0, 3],\n",
       "          [2, 1]]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1) # Get top-k experts\n",
    "top_k_logits, top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2d979",
   "metadata": {},
   "source": [
    "## Create Sparse Logits\n",
    "Creates a sparse representation by filling a tensor with -inf and scattering only the top-k logit values back. This ensures only selected experts will have non-zero weights after softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "389e953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  -inf,   -inf, 0.4986, 0.3198],\n",
       "         [  -inf, 0.1188, 0.2375,   -inf],\n",
       "         [0.3512,   -inf,   -inf, 0.2058],\n",
       "         [  -inf, 0.2521, 0.6194,   -inf]]], grad_fn=<ScatterBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.full_like(logits, float('-inf')) #full_like clones a tensor and fills it with a specified\n",
    "sparse_logits = zeros.scatter(-1, top_k_indices, top_k_logits)\n",
    "sparse_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e81a9a6",
   "metadata": {},
   "source": [
    "## Apply Softmax for Gating Weights\n",
    "Applies softmax to the sparse logits to produce final gating weights. Non-selected experts (with -inf logits) will have zero probability, implementing sparse routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2f11dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.5446, 0.4554],\n",
       "         [0.0000, 0.4704, 0.5296, 0.0000],\n",
       "         [0.5363, 0.0000, 0.0000, 0.4637],\n",
       "         [0.0000, 0.4092, 0.5908, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_output= F.softmax(sparse_logits, dim=-1)\n",
    "gating_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494fe43",
   "metadata": {},
   "source": [
    "## Define TopkRouter Class\n",
    "Implements the complete top-k routing mechanism as a PyTorch module. Takes multi-head attention output and routes it to the top-k experts based on learned linear transformation and softmax selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ce9897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopkRouter (nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, num_experts, top_k) :\n",
    "        super (TopkRouter, self). __init__()\n",
    "        self.top_k = top_k\n",
    "        self. linear =nn. Linear(n_embed, num_experts)\n",
    "    def forward (self, mh_ouput) :\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self. linear (mh_output)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax (sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb26c5",
   "metadata": {},
   "source": [
    "## Test TopkRouter\n",
    "Tests the TopkRouter with 3 experts, top-k=2, and embedding dimension of 8. Demonstrates the shape of outputs and shows which experts are selected for each token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c236e91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 3]),\n",
       " tensor([[[0.0000, 0.7829, 0.2171],\n",
       "          [0.7314, 0.0000, 0.2686],\n",
       "          [0.5791, 0.0000, 0.4209],\n",
       "          [0.7519, 0.0000, 0.2481]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[1, 2],\n",
       "          [0, 2],\n",
       "          [0, 2],\n",
       "          [0, 2]]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experts = 3\n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "mh_output = torch. randn(1, 4, n_embd) # Example input\n",
    "top_k_gate = TopkRouter (n_embd, num_experts,top_k)\n",
    "gating_output, indices = top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931482ba",
   "metadata": {},
   "source": [
    "## Noisy Top-K Gating\n",
    "\n",
    "Noisy top-k gating is an important technique for training Mixture of Experts models effectively.\n",
    "\n",
    "**Problem Without Noisy Gating:**\n",
    "- All tokens tend to route to the same set of \"favored\" experts (the ones with highest logits)\n",
    "- This causes load imbalance - some experts are overused while others are underutilized\n",
    "- The model becomes inefficient and wastes computational resources\n",
    "\n",
    "**Solution - Noisy Top-K Gating:**\n",
    "- Add Gaussian noise to the logits from the gating linear layer during training\n",
    "- This encourages random exploration of different expert combinations\n",
    "- Creates a balance between **exploitation** (using the best experts) and **exploration** (trying other experts)\n",
    "\n",
    "**Benefits:**\n",
    "1. **Load Balancing**: Distributes tokens more evenly across all experts\n",
    "2. **Better Training**: Prevents experts from becoming inactive or redundant\n",
    "3. **Improved Performance**: Leads to more efficient and effective MoE models\n",
    "4. **Regularization**: Acts as a form of regularization during training\n",
    "\n",
    "**Implementation:**\n",
    "During training, add noise to logits: `logits_noisy = logits + Gaussian_noise`\n",
    "During inference, use the clean logits without noise for deterministic expert selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3695ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopkRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements noisy top-k routing for Mixture of Experts.\n",
    "    Adds Gaussian noise to logits during training to encourage load balancing.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.num_experts = num_experts\n",
    "        # Layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        # Layer for noise scaling\n",
    "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
    "    \n",
    "    def forward(self, mh_output, training=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mh_output: Output from multihead self-attention block\n",
    "            training: Whether in training mode (adds noise) or inference mode\n",
    "        \n",
    "        Returns:\n",
    "            router_output: Gating weights for expert selection\n",
    "            indices: Indices of top-k selected experts\n",
    "        \"\"\"\n",
    "        # Get logits from the router linear layer\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "        \n",
    "        if training:\n",
    "            # Get noise scaling logits\n",
    "            noise_logits = self.noise_linear(mh_output)\n",
    "            # Add scaled unit gaussian noise to the logits\n",
    "            # F.softplus ensures noise scaling is positive\n",
    "            noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
    "            noisy_logits = logits + noise\n",
    "        else:\n",
    "            # Use clean logits during inference\n",
    "            noisy_logits = logits\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        \n",
    "        # Create sparse logits (set non-top-k to -inf)\n",
    "        zeros = torch.full_like(logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        \n",
    "        # Apply softmax for gating weights\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        \n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7691cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 4]),\n",
       " tensor([[[0.4249, 0.0000, 0.5751, 0.0000],\n",
       "          [0.7868, 0.0000, 0.0000, 0.2132],\n",
       "          [0.2520, 0.0000, 0.7480, 0.0000],\n",
       "          [0.8716, 0.1284, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[2, 0],\n",
       "          [0, 3],\n",
       "          [2, 0],\n",
       "          [0, 1]]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test NoisyTopkRouter\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "mh_output = torch.randn(1, 4, n_embd)  # Batch size 2, sequence length 4\n",
    "\n",
    "# Create router\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output,indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea7b51",
   "metadata": {},
   "source": [
    "## Create the Sparse Mixture of Experts (MoE)\n",
    "\n",
    "![MoE Architecture](assets/MoE_enhanced.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a72358",
   "metadata": {},
   "source": [
    "## SparseMoE Block Output Computation\n",
    "\n",
    "The SparseMoE (Sparse Mixture of Experts) block's output is computed through a multi-step process:\n",
    "\n",
    "### (a) Expert Selector Weight Matrix\n",
    "The primary aspect of this process involves the **expert selector weight matrix**, which is generated by the router/gating mechanism. This weight matrix contains:\n",
    "- One row per token in the sequence\n",
    "- One column per expert in the mixture\n",
    "- Values representing the gating weights for routing each token to experts\n",
    "- Only **top-k experts have non-zero weights** (others are zero due to sparse routing)\n",
    "\n",
    "### (b) Top-K Expert Output Multiplication\n",
    "After acquiring the expert selector weight matrix, the **top-k gating weights are selectively multiplied** with the outputs from the corresponding top-k experts for each token:\n",
    "\n",
    "$$\\text{weighted\\_expert\\_output} = \\text{gating\\_weight} \\times \\text{expert\\_output}$$\n",
    "\n",
    "For each token, only the k experts with the highest gating weights process that token, making the computation sparse and efficient.\n",
    "\n",
    "### (c) Weighted Sum - SparseMoE Output\n",
    "This selective multiplication of gating weights with expert outputs forms a **weighted sum**, which constitutes the SparseMoE block's final output:\n",
    "\n",
    "$$\\text{SparseMoE\\_output} = \\sum_{i \\in \\text{top-k experts}} \\text{gating\\_weight}_i \\times \\text{expert\\_output}_i$$\n",
    "\n",
    "Where:\n",
    "- The sum iterates only over the selected top-k experts\n",
    "- Each expert's output is weighted by its corresponding gating weight\n",
    "- The result is a single output vector per token that combines information from multiple experts\n",
    "- This weighted sum effectively blends the knowledge of multiple specialized experts for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7556a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse Mixture of Experts module that routes tokens to top-k experts\n",
    "    and combines their outputs using learned gating weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, num_experts, top_k, dropout=0.1):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed, dropout) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, n_embed)\n",
    "            training: Whether in training mode (affects gating noise)\n",
    "        \n",
    "        Returns:\n",
    "            final_output: Weighted combination of expert outputs\n",
    "        \"\"\"\n",
    "        # Get gating weights and expert indices from router\n",
    "        gating_output, indices = self.router(x, training=training)\n",
    "        \n",
    "        # Initialize output tensor with same shape as input\n",
    "        final_output = torch.zeros_like(x)\n",
    "        \n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "        \n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "            \n",
    "            if flat_mask.any():\n",
    "                # Extract inputs for this expert\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                \n",
    "                # Process through expert\n",
    "                expert_output = expert(expert_input)\n",
    "                \n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "                \n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebe51443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 8])\n",
      "Output shape: torch.Size([2, 4, 8])\n",
      "Output:\n",
      "tensor([[[-0.1479,  0.1550, -0.0756, -0.1690, -0.0151, -0.0343, -0.1801,\n",
      "           0.0772],\n",
      "         [-0.1597,  0.2710,  0.0299,  0.0714, -0.0486,  0.0669, -0.5236,\n",
      "          -0.2877],\n",
      "         [ 0.0617, -0.0243, -0.0955,  0.0070,  0.0805, -0.0490, -0.3016,\n",
      "          -0.0283],\n",
      "         [ 0.1945, -0.1716,  0.1555, -0.0450, -0.0152,  0.2378, -0.0966,\n",
      "          -0.0355]],\n",
      "\n",
      "        [[ 0.3903,  0.3048, -0.1689,  0.1523,  0.0050, -0.0926, -0.1269,\n",
      "          -0.1720],\n",
      "         [ 0.0941,  0.1254, -0.1340, -0.1856, -0.0643, -0.0652, -0.1836,\n",
      "           0.0155],\n",
      "         [-0.2607,  0.5290,  0.2636, -0.1587,  0.4651,  0.4380, -0.2503,\n",
      "          -0.0622],\n",
      "         [ 0.4173, -0.0921,  0.2127, -0.0892,  0.1658, -0.0387, -0.3208,\n",
      "          -0.0671]]], grad_fn=<IndexPutBackward0>)\n",
      "\n",
      "Shape matches input: True\n"
     ]
    }
   ],
   "source": [
    "# Test SparseMoE\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "dropout = 0.1\n",
    "\n",
    "# Create SparseMoE block\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k, dropout)\n",
    "\n",
    "# Create sample input\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "x = torch.randn(batch_size, seq_len, n_embd)\n",
    "\n",
    "# Forward pass\n",
    "output = sparse_moe(x, training=True)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "print(f\"\\nShape matches input: {output.shape == x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d1742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
